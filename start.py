#!/usr/bin/env python3
"""
ä¸­æ–‡å°è¯´å†™ä½œAIç³»ç»Ÿ - ä¸»å¯åŠ¨å…¥å£

æ”¯æŒæ¨¡å¼:
1. webui - å¯åŠ¨Webç•Œé¢ (é»˜è®¤)
2. train - å¯åŠ¨è®­ç»ƒ
3. prepare - å‡†å¤‡è®­ç»ƒæ•°æ®
4. inference - å‘½ä»¤è¡Œæ¨ç†æµ‹è¯•
5. convert - æ¨¡å‹æ ¼å¼è½¬æ¢ (HF â†’ GGUF, LoRA â†’ GGUF)
"""
import os
import sys
import argparse
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))


def launch_webui(args):
    """å¯åŠ¨WebUI"""
    import asyncio
    from src.webui.app import launch_webui
    from config import config

    lora_path = args.lora if hasattr(args, 'lora') and args.lora else None

    # åº”ç”¨å‘½ä»¤è¡ŒæŒ‡å®šçš„åŸºç¡€æ¨¡å‹
    if hasattr(args, 'base_model') and args.base_model:
        config.model.base_model = args.base_model

    # åº”ç”¨ llama.cpp æ¨¡å‹æ ¼å¼é…ç½®
    if hasattr(args, 'model_format') and args.model_format:
        config.model.llama_cpp_model_format = args.model_format
        # åŒæ—¶æ›´æ–° HF æ¨¡å‹è·¯å¾„ä»¥åŒ¹é…
        if args.model_format == "hf":
            config.model.llama_cpp_hf_model = args.base_model or config.model.base_model

    print("=" * 60)
    print("ğŸš€ å¯åŠ¨ä¸­æ–‡å°è¯´å†™ä½œAIç³»ç»Ÿ - WebUI")
    print("=" * 60)
    print(f"ğŸ“¦ åŸºç¡€æ¨¡å‹: {config.model.base_model}")

    asyncio.run(launch_webui(lora_path=lora_path))


def launch_train(args):
    """å¯åŠ¨è®­ç»ƒ"""
    from src.train.train_lora import main as train_main
    from config import config

    # åº”ç”¨å‘½ä»¤è¡ŒæŒ‡å®šçš„åŸºç¡€æ¨¡å‹
    if hasattr(args, 'base_model') and args.base_model:
        config.model.base_model = args.base_model

    print("=" * 60)
    print("ğŸ¯ å¯åŠ¨æ¨¡å‹å¾®è°ƒè®­ç»ƒ")
    print("=" * 60)
    print(f"ğŸ“¦ åŸºç¡€æ¨¡å‹: {config.model.base_model}")

    # æ”¶é›†è®­ç»ƒå‚æ•°
    train_kwargs = {}
    if hasattr(args, 'resume') and args.resume:
        train_kwargs['resume_from_checkpoint'] = args.resume
    if hasattr(args, 'train_data') and args.train_data:
        train_kwargs['train_data_path'] = args.train_data
    if hasattr(args, 'val_data') and args.val_data:
        train_kwargs['val_data_path'] = args.val_data
    if hasattr(args, 'output_dir') and args.output_dir:
        train_kwargs['output_dir'] = args.output_dir
    if hasattr(args, 'checkpoint_dir') and args.checkpoint_dir:
        train_kwargs['checkpoint_dir'] = args.checkpoint_dir
    if hasattr(args, 'epochs') and args.epochs:
        train_kwargs['num_train_epochs'] = args.epochs
    if hasattr(args, 'batch_size') and args.batch_size:
        train_kwargs['per_device_train_batch_size'] = args.batch_size
    if hasattr(args, 'lr') and args.lr:
        train_kwargs['learning_rate'] = args.lr

    train_main(**train_kwargs)


def prepare_data(args):
    """å‡†å¤‡æ•°æ®"""
    from src.data.prepare_data import NovelDataPreparer

    print("=" * 60)
    print("ğŸ“š å‡†å¤‡è®­ç»ƒæ•°æ®")
    print("=" * 60)

    preparer = NovelDataPreparer(args.data_dir or "./training/data")

    if args.sample:
        preparer.create_sample_data()
    else:
        stats = preparer.prepare_training_data(
            chunk_size=args.chunk_size,
            val_split=args.val_split,
            min_length=args.min_length,
        )
        print(f"\nâœ“ å®Œæˆ! è®­ç»ƒé›†: {stats['train']} | éªŒè¯é›†: {stats['val']}")


def run_inference(args):
    """è¿è¡Œæ¨ç†æµ‹è¯•"""
    from config import config
    backend = config.model.inference_backend

    print("=" * 60)
    print("ğŸ¤– è¿è¡Œæ¨ç†æµ‹è¯•")
    print(f"ğŸ“Š æ¨ç†åç«¯: {backend}")
    print("=" * 60)

    if backend == "vllm":
        import asyncio
        from src.inference.vllm_server import main as inference_main
        asyncio.run(inference_main())
    elif backend == "llama_cpp":
        from src.inference.llama_server import main as inference_main
        inference_main()
    else:
        print(f"âŒ ä¸æ”¯æŒçš„æ¨ç†åç«¯: {backend}")
        print("æ”¯æŒçš„åç«¯: vllm, llama_cpp")
        sys.exit(1)


def convert_model(args):
    """æ¨¡å‹æ ¼å¼è½¬æ¢"""
    import subprocess
    from pathlib import Path

    script_dir = Path(__file__).parent / "scripts"

    print("=" * 60)
    print("ğŸ”„ æ¨¡å‹æ ¼å¼è½¬æ¢")
    print("=" * 60)

    if args.convert_type == "hf-to-gguf":
        # Hugging Face â†’ GGUF
        script = script_dir / "convert_hf_to_gguf.sh"
        model = args.model or "Qwen/Qwen2.5-7B-Instruct"
        quant = args.quant or "Q5_K_M"

        print(f"ğŸ“¦ è½¬æ¢: {model}")
        print(f"ğŸ“Š é‡åŒ–ç±»å‹: {quant}")
        print()

        subprocess.run([str(script), model, quant], check=True)

    elif args.convert_type == "lora-to-gguf":
        # LoRA â†’ GGUF
        script = script_dir / "convert_lora_to_gguf.sh"
        base_model = args.base_model or "Qwen/Qwen2.5-7B-Instruct"
        lora_path = args.lora_path or "./training/final_model"
        output_dir = args.output_dir or "./models/lora-gguf"

        print(f"ğŸ“¦ åŸºç¡€æ¨¡å‹: {base_model}")
        print(f"ğŸ“¦ LoRA è·¯å¾„: {lora_path}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
        print()

        subprocess.run([str(script), base_model, lora_path, output_dir], check=True)

    else:
        print(f"âŒ ä¸æ”¯æŒçš„è½¬æ¢ç±»å‹: {args.convert_type}")
        print("æ”¯æŒçš„ç±»å‹: hf-to-gguf, lora-to-gguf")
        sys.exit(1)


def main():
    """ä¸»å…¥å£"""
    parser = argparse.ArgumentParser(
        description="ä¸­æ–‡å°è¯´å†™ä½œAIç³»ç»Ÿ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # å¯åŠ¨WebUI (é»˜è®¤æ¨¡å‹)
  python start.py

  # å¯åŠ¨WebUI (æŒ‡å®šQwenåŸºç¡€æ¨¡å‹)
  python start.py webui --base-model Qwen/Qwen2.5-7B-Instruct

  # å…¶ä»–Qwenæ¨¡å‹é€‰é¡¹:
  # Qwen2.5ç³»åˆ—: 0.5B, 1.5B, 3B, 7B, 14B, 32B, 72B
  python start.py webui --base-model Qwen/Qwen2.5-3B-Instruct
  python start.py webui --base-model Qwen/Qwen2.5-14B-Instruct
  python start.py webui --base-model Qwen/Qwen2.5-32B-Instruct
  python start.py webui --base-model Qwen/Qwen2.5-72B-Instruct

  # Qwen2ç³»åˆ—
  python start.py webui --base-model Qwen/Qwen2-7B-Instruct
  python start.py webui --base-model Qwen/Qwen2-72B-Instruct

  # å¯åŠ¨WebUI (æŒ‡å®šåŸºç¡€æ¨¡å‹ + LoRAæƒé‡)
  python start.py webui --base-model Qwen/Qwen2.5-7B-Instruct --lora ./training/final_model

  # å‡†å¤‡è®­ç»ƒæ•°æ®
  python start.py prepare --sample

  # å¼€å§‹è®­ç»ƒ (é»˜è®¤æ¨¡å‹)
  python start.py train

  # å¼€å§‹è®­ç»ƒ (æŒ‡å®šåŸºç¡€æ¨¡å‹)
  python start.py train --base-model Qwen/Qwen2.5-7B-Instruct

  # å¼€å§‹è®­ç»ƒ (æŒ‡å®šæ•°æ®å’Œè¾“å‡ºç›®å½•)
  # ç¤ºä¾‹: ä½¿ç”¨ urban-novels æ•°æ®é›†
  # python start.py train \\
  #   --base-model Qwen/Qwen2.5-7B-Instruct \\
  #   --train-data ./data/train_urban-novels/train.jsonl \\
  #   --val-data ./data/val_urban-novels/val.jsonl \\
  #   --output-dir ./training/urban-novels_model \\
  #   --epochs 3

  # æ¨ç†æµ‹è¯•
  python start.py inference

Qwenæ¨¡å‹ç³»åˆ— (æ¨è):
  - Qwen/Qwen2.5-0.5B-Instruct   (æœ€å°, ~1GBæ˜¾å­˜)
  - Qwen/Qwen2.5-1.5B-Instruct   (å°å‹, ~3GBæ˜¾å­˜)
  - Qwen/Qwen2.5-3B-Instruct    (ä¸­å‹, ~6GBæ˜¾å­˜)
  - Qwen/Qwen2.5-7B-Instruct    (æ¨è, ~14GBæ˜¾å­˜)
  - Qwen/Qwen2.5-14B-Instruct   (å¤§å‹, ~28GBæ˜¾å­˜)
  - Qwen/Qwen2.5-32B-Instruct   (è¶…å¤§å‹, ~64GBæ˜¾å­˜)
  - Qwen/Qwen2.5-72B-Instruct   (æœ€å¤§, ~128GBæ˜¾å­˜)

CPU æ¨ç† (llama.cpp) æ¨¡å‹æ ¼å¼:
  --model-format gguf  ä½¿ç”¨ GGUF é‡åŒ–æ¨¡å‹ (æ¨è, å†…å­˜å ç”¨å°)
  --model-format hf    ä½¿ç”¨ Hugging Face éé‡åŒ–æ¨¡å‹ (ç²¾åº¦é«˜, å†…å­˜å ç”¨å¤§)

  ç¤ºä¾‹:
  # CPU æ¨ç† - ä½¿ç”¨é‡åŒ–æ¨¡å‹ (é»˜è®¤)
  python start.py webui --model-format gguf

  # CPU æ¨ç† - ä½¿ç”¨éé‡åŒ–æ¨¡å‹ (ç²¾åº¦æ›´é«˜)
  python start.py webui --model-format hf --base-model Qwen/Qwen2.5-3B-Instruct
        """,
    )

    subparsers = parser.add_subparsers(dest="mode", help="è¿è¡Œæ¨¡å¼")

    # WebUIæ¨¡å¼
    webui_parser = subparsers.add_parser("webui", help="å¯åŠ¨Webç•Œé¢")
    webui_parser.add_argument("--base-model", type=str, default=None, help="åŸºç¡€æ¨¡å‹åç§° (å¦‚: Qwen/Qwen2.5-7B-Instruct)")
    webui_parser.add_argument("--model-format", type=str, default=None, choices=["gguf", "hf"], help="CPUæ¨ç†æ¨¡å‹æ ¼å¼: gguf(é‡åŒ–) æˆ– hf(éé‡åŒ–)")
    webui_parser.add_argument("--lora", type=str, default=None, help="LoRAæƒé‡è·¯å¾„")
    webui_parser.add_argument("--host", type=str, default=None, help="æœåŠ¡å™¨åœ°å€")
    webui_parser.add_argument("--port", type=int, default=None, help="æœåŠ¡å™¨ç«¯å£")
    webui_parser.add_argument("--share", action="store_true", help="åˆ›å»ºå…¬å…±é“¾æ¥")

    # è®­ç»ƒæ¨¡å¼
    train_parser = subparsers.add_parser("train", help="å¯åŠ¨æ¨¡å‹è®­ç»ƒ")
    train_parser.add_argument("--base-model", type=str, default=None, help="åŸºç¡€æ¨¡å‹åç§° (å¦‚: Qwen/Qwen2.5-7B-Instruct)")
    train_parser.add_argument("--data", type=str, default=None, help="è®­ç»ƒæ•°æ®è·¯å¾„")
    train_parser.add_argument("--train-data", type=str, default=None, help="è®­ç»ƒæ•°æ®è·¯å¾„ (JSONL)")
    train_parser.add_argument("--val-data", type=str, default=None, help="éªŒè¯æ•°æ®è·¯å¾„ (JSONL)")
    train_parser.add_argument("--epochs", type=int, default=None, help="è®­ç»ƒè½®æ•°")
    train_parser.add_argument("--batch-size", type=int, default=None, help="æ‰¹æ¬¡å¤§å°")
    train_parser.add_argument("--lr", type=float, default=None, help="å­¦ä¹ ç‡")
    train_parser.add_argument("--resume", type=str, default=None, help="ä»checkpointæ¢å¤è®­ç»ƒ")
    train_parser.add_argument("--output-dir", type=str, default=None, help="è¾“å‡ºç›®å½•")
    train_parser.add_argument("--checkpoint-dir", type=str, default=None, help="æ£€æŸ¥ç‚¹ç›®å½•")

    # æ•°æ®å‡†å¤‡æ¨¡å¼
    prepare_parser = subparsers.add_parser("prepare", help="å‡†å¤‡è®­ç»ƒæ•°æ®")
    prepare_parser.add_argument("--data-dir", type=str, default="./training/data", help="æ•°æ®ç›®å½•")
    prepare_parser.add_argument("--chunk-size", type=int, default=2048, help="è®­ç»ƒå—å¤§å°")
    prepare_parser.add_argument("--val-split", type=float, default=0.1, help="éªŒè¯é›†æ¯”ä¾‹")
    prepare_parser.add_argument("--min-length", type=int, default=500, help="æœ€å°æ–‡æœ¬é•¿åº¦")
    prepare_parser.add_argument("--sample", action="store_true", help="åˆ›å»ºç¤ºä¾‹æ•°æ®")

    # æ¨ç†æ¨¡å¼
    inference_parser = subparsers.add_parser("inference", help="è¿è¡Œæ¨ç†æµ‹è¯•")

    # è½¬æ¢æ¨¡å¼
    convert_parser = subparsers.add_parser("convert", help="æ¨¡å‹æ ¼å¼è½¬æ¢")
    convert_subparsers = convert_parser.add_subparsers(dest="convert_type", help="è½¬æ¢ç±»å‹")

    # HF â†’ GGUF è½¬æ¢
    hf_gguf_parser = convert_subparsers.add_parser("hf-to-gguf", help="Hugging Face æ¨¡å‹è½¬æ¢ä¸º GGUF æ ¼å¼")
    hf_gguf_parser.add_argument("--model", type=str, default=None, help="Hugging Face æ¨¡å‹åç§°")
    hf_gguf_parser.add_argument("--quant", type=str, default=None, help="é‡åŒ–ç±»å‹ (Q5_K_M, Q8_0, etc.)")

    # LoRA â†’ GGUF è½¬æ¢
    lora_gguf_parser = convert_subparsers.add_parser("lora-to-gguf", help="LoRA æƒé‡è½¬æ¢ä¸º GGUF æ ¼å¼")
    lora_gguf_parser.add_argument("--base-model", type=str, default=None, help="åŸºç¡€æ¨¡å‹åç§°")
    lora_gguf_parser.add_argument("--lora-path", type=str, default=None, help="LoRA æƒé‡è·¯å¾„")
    lora_gguf_parser.add_argument("--output-dir", type=str, default=None, help="è¾“å‡ºç›®å½•")

    args = parser.parse_args()

    # é»˜è®¤å¯åŠ¨WebUI
    if args.mode is None:
        args.mode = "webui"

    # æ ¹æ®æ¨¡å¼å¯åŠ¨
    if args.mode == "webui":
        launch_webui(args)
    elif args.mode == "train":
        launch_train(args)
    elif args.mode == "prepare":
        prepare_data(args)
    elif args.mode == "inference":
        run_inference(args)
    elif args.mode == "convert":
        convert_model(args)
    else:
        parser.print_help()


if __name__ == "__main__":
    main()
