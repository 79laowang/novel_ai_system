# ä¸­æ–‡å°è¯´å†™ä½œAIç³»ç»Ÿ

åŸºäº **vLLM/llama.cpp + LoRA + ChromaDB** çš„æ™ºèƒ½å°è¯´åˆ›ä½œç³»ç»Ÿï¼Œæ”¯æŒè®­ç»ƒã€æ¨ç†å’Œè®°å¿†ç®¡ç†ã€‚

## âœ¨ ç‰¹æ€§

- **åŒæ¨ç†åç«¯**: æ”¯æŒ GPU (vLLM) å’Œ CPU (llama.cpp) æ¨ç†
- **é«˜æ€§èƒ½æ¨ç†**: GPU ä¸Šä½¿ç”¨ vLLMï¼ŒCPU ä¸Šä½¿ç”¨ llama.cpp
- **LoRA å¾®è°ƒ**: QLoRA 4-bit é‡åŒ–è®­ç»ƒï¼Œé™ä½æ˜¾å­˜éœ€æ±‚
- **è®°å¿†åŠŸèƒ½**: å‘é‡æ•°æ®åº“å­˜å‚¨ï¼Œæ”¯æŒé•¿æœŸè®°å¿†å’Œ RAG
- **WebUI ç•Œé¢**: Gradio æ„å»ºï¼Œæ˜“äºä½¿ç”¨
- **ä¸­æ–‡ä¼˜åŒ–**: ä¸“ä¸ºä¸­æ–‡å°è¯´å†™ä½œä¼˜åŒ–

## ğŸ“‹ ç³»ç»Ÿè¦æ±‚

### GPU è®­ç»ƒæœºå™¨
- Python 3.8+
- CUDA 12.0+
- GPU: å»ºè®® 16GB+ æ˜¾å­˜ (RTX 4090 / A100 ç­‰)
- å†…å­˜: å»ºè®® 64GB+

### CPU æ¨ç†æœºå™¨
- Python 3.8+
- å†…å­˜: å»ºè®® 22GB+ (7B æ¨¡å‹ï¼ŒQ5 é‡åŒ–)
- CPU: å»ºè®® 6 æ ¸å¿ƒä»¥ä¸Š

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹å¼ä¸€ï¼šGPU è®­ç»ƒ + GPU æ¨ç†

```bash
# 1. å®‰è£…ä¾èµ–
pip install -r requirements.txt

# 2. å‡†å¤‡è®­ç»ƒæ•°æ®
python start.py prepare --sample

# 3. è®­ç»ƒæ¨¡å‹
python start.py train

# 4. å¯åŠ¨ WebUI (ä½¿ç”¨ vLLM)
python start.py webui
```

### æ–¹å¼äºŒï¼šGPU è®­ç»ƒ + CPU æ¨ç† â­ æ¨è

```bash
# GPU æœºå™¨ä¸Šï¼š
# 1. å®‰è£…ä¾èµ–
pip install -r requirements.txt

# 2. å‡†å¤‡è®­ç»ƒæ•°æ®
python start.py prepare --sample

# 3. è®­ç»ƒæ¨¡å‹
python start.py train

# 4. è½¬æ¢æ¨¡å‹ä¸º GGUF æ ¼å¼
python start.py convert hf-to-gguf --model Qwen/Qwen2.5-7B-Instruct --quant Q5_K_M
python start.py convert lora-to-gguf --lora-path ./checkpoints/final_model

# 5. å°†æ¨¡å‹æ–‡ä»¶å¤åˆ¶åˆ° CPU æœºå™¨
# ./models/qwen2.5-7b-q5_k_m.gguf
# ./models/lora-gguf/

# CPU æœºå™¨ä¸Šï¼š
# 1. å®‰è£…ä¾èµ– (ä¸éœ€è¦ torch/vllm)
pip install llama-cpp-python gradio chromadb langchain sentence-transformers

# 2. ä¿®æ”¹ config.py
# inference_backend: str = "llama_cpp"
# llama_cpp_model_path: str = "./models/qwen2.5-7b-q5_k_m.gguf"
# llama_cpp_lora_path: str = "./models/lora-gguf"

# 3. å¯åŠ¨ WebUI (ä½¿ç”¨ llama.cpp)
python start.py webui
```

## ğŸ“ è®­ç»ƒå®Œæ•´æŒ‡å—

### è®­ç»ƒæµç¨‹æ¦‚è§ˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        è®­ç»ƒæµç¨‹                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

   1. å‡†å¤‡æ•°æ®          2. é…ç½®å‚æ•°          3. å¼€å§‹è®­ç»ƒ
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ å°è¯´æ–‡ä»¶ â”‚ â”€â”€â”€â”€â”€â–º â”‚ config.pyâ”‚ â”€â”€â”€â”€â”€â–º â”‚ è®­ç»ƒä¸­  â”‚
   â”‚.txt/jsonâ”‚         â”‚ è°ƒæ•´å‚æ•° â”‚         â”‚ ç›‘æ§lossâ”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                                                    â”‚
   5. ä½¿ç”¨æ¨¡å‹          4. æ¢å¤è®­ç»ƒ              â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
   â”‚ WebUI   â”‚ â—„â”€â”€â”€â”€â”€â”€ â”‚ä¸­æ–­å   â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â”‚ ç”Ÿæˆ    â”‚         â”‚æ¢å¤è®­ç»ƒ â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ­¥éª¤ 1: å‡†å¤‡è®­ç»ƒæ•°æ®

#### æ”¯æŒçš„æ•°æ®æ ¼å¼

```bash
# æ–¹å¼ä¸€ï¼šTXT æ–‡æœ¬æ–‡ä»¶ï¼ˆæœ€ç®€å•ï¼‰
# ç›´æ¥å°†å°è¯´æ–‡ä»¶æ”¾å…¥ data/raw/ ç›®å½•
cp my_novel.txt data/raw/

# æ–¹å¼äºŒï¼šJSON æ ¼å¼
[
  {"text": "å°è¯´å†…å®¹ç¬¬ä¸€æ®µ..."},
  {"text": "å°è¯´å†…å®¹ç¬¬äºŒæ®µ..."}
]

# æ–¹å¼ä¸‰ï¼šJSONL æ ¼å¼ï¼ˆæ¨èå¤§è§„æ¨¡æ•°æ®ï¼‰
{"text": "å†…å®¹1..."}
{"text": "å†…å®¹2..."}
```

#### æ•°æ®å‡†å¤‡å‘½ä»¤

```bash
# åˆ›å»ºç¤ºä¾‹æ•°æ®ï¼ˆç”¨äºæµ‹è¯•ï¼‰
python start.py prepare --sample

# å‡†å¤‡è‡ªå®šä¹‰æ•°æ®
python start.py prepare --chunk-size 2048 --val-split 0.1

# å‚æ•°è¯´æ˜ï¼š
# --chunk-size    æ¯ä¸ªè®­ç»ƒæ ·æœ¬çš„æœ€å¤§tokenæ•° (é»˜è®¤: 2048)
# --val-split     éªŒè¯é›†æ¯”ä¾‹ (é»˜è®¤: 0.1ï¼Œå³10%)
# --min-length    æœ€å°æ–‡æœ¬é•¿åº¦ï¼Œè¿‡æ»¤å¤ªçŸ­çš„å†…å®¹ (é»˜è®¤: 500)
```

#### æ•°æ®è´¨é‡å»ºè®®

| æŒ‡æ ‡ | å»ºè®®å€¼ | è¯´æ˜ |
|------|--------|------|
| æ•°æ®é‡ | 10MB+ | è¶Šå¤šè¶Šå¥½ï¼Œå»ºè®®è‡³å°‘å‡ MBçº¯æ–‡æœ¬ |
| æ–‡æœ¬è´¨é‡ | é«˜ | å»é™¤HTMLæ ‡ç­¾ã€ä¹±ç ã€æ— å…³å†…å®¹ |
| å†…å®¹ä¸€è‡´æ€§ | å•ä¸€é£æ ¼ | åŒä¸€é£æ ¼/ä½œè€…/é¢˜ææ•ˆæœæ›´å¥½ |
| éªŒè¯é›†æ¯”ä¾‹ | 0.05-0.1 | æ•°æ®å°‘æ—¶å¯é™ä½æ¯”ä¾‹ |

### æ­¥éª¤ 2: é…ç½®è®­ç»ƒå‚æ•°

ç¼–è¾‘ `config.py` ä¸­çš„ `TrainingConfig` éƒ¨åˆ†ï¼š

```python
@dataclass
class TrainingConfig:
    # === æ•°æ®è·¯å¾„ ===
    train_data_path: str = "data/train/train.jsonl"  # è®­ç»ƒæ•°æ®
    val_data_path: str = "data/val/val.jsonl"        # éªŒè¯æ•°æ®
    max_seq_length: int = 2048                       # æœ€å¤§åºåˆ—é•¿åº¦

    # === è®­ç»ƒå‚æ•° ===
    num_train_epochs: int = 3                        # è®­ç»ƒè½®æ•°
    per_device_train_batch_size: int = 1             # æ¯GPUæ‰¹æ¬¡å¤§å°
    per_device_eval_batch_size: int = 1              # è¯„ä¼°æ‰¹æ¬¡å¤§å°
    gradient_accumulation_steps: int = 4             # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
    learning_rate: float = 2e-4                      # å­¦ä¹ ç‡
    warmup_steps: int = 100                          # é¢„çƒ­æ­¥æ•°

    # === ä¿å­˜å’Œæ—¥å¿— ===
    logging_steps: int = 10                          # æ—¥å¿—è®°å½•é¢‘ç‡
    save_steps: int = 500                            # checkpointä¿å­˜é¢‘ç‡
    eval_steps: int = 500                            # è¯„ä¼°é¢‘ç‡

    # === ä¼˜åŒ–å™¨ ===
    optimizer: str = "paged_adamw_32bit"             # ä¼˜åŒ–å™¨ç±»å‹
    weight_decay: float = 0.01                       # æƒé‡è¡°å‡
    max_grad_norm: float = 1.0                       # æ¢¯åº¦è£å‰ª

    # === å…¶ä»– ===
    bf16: bool = True                                # ä½¿ç”¨bfloat16
    fp16: bool = False                               # ä½¿ç”¨float16
    gradient_checkpointing: bool = True              # æ¢¯åº¦æ£€æŸ¥ç‚¹(çœæ˜¾å­˜)
```

### æ­¥éª¤ 3: å‚æ•°è°ƒæ•´æŒ‡å—

#### æ ¹æ®æ˜¾å­˜è°ƒæ•´å‚æ•°

| æ˜¾å­˜å¤§å° | batch_size | gradient_accumulation | max_seq_length | é‡åŒ– |
|----------|------------|----------------------|----------------|------|
| 8GB      | 1          | 8                    | 1024           | 4-bit |
| 12GB     | 1          | 4                    | 2048           | 4-bit |
| 16GB     | 2          | 4                    | 2048           | 4-bit |
| 24GB     | 4          | 2                    | 4096           | 8-bit/æ—  |
| 40GB+    | 8          | 1                    | 8192           | æ—  |

**æœ‰æ•ˆæ‰¹æ¬¡å¤§å°è®¡ç®—**ï¼š
```
æœ‰æ•ˆæ‰¹æ¬¡ = batch_size Ã— gradient_accumulation Ã— GPUæ•°é‡
```

#### æ ¹æ®æ•°æ®é‡è°ƒæ•´è®­ç»ƒè½®æ•°

| æ•°æ®é‡ | æ¨èè½®æ•° | è¯´æ˜ |
|--------|----------|------|
| < 1MB   | 5-10     | æ•°æ®å°‘éœ€è¦å¤šè½®æ¬¡ |
| 1-10MB  | 3-5      | æ ‡å‡† |
| 10-100MB| 2-3      | æ•°æ®å……è¶³ |
| > 100MB | 1-2      | å¤§æ•°æ®é›† |

#### å­¦ä¹ ç‡è°ƒæ•´

```python
# é»˜è®¤å­¦ä¹ ç‡
learning_rate = 2e-4  # é€‚ç”¨äºå¤§å¤šæ•°æƒ…å†µ

# è®­ç»ƒä¸ç¨³å®šæ—¶ï¼ˆlosséœ‡è¡ï¼‰
learning_rate = 1e-4  # é™ä½å­¦ä¹ ç‡

# è®­ç»ƒå¤ªæ…¢æ—¶
learning_rate = 5e-4  # æé«˜å­¦ä¹ ç‡ï¼ˆè°¨æ…ï¼‰

# ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨
warmup_steps = int(total_steps * 0.1)  # é¢„çƒ­10%çš„æ­¥æ•°
```

#### LoRA å‚æ•°è°ƒæ•´

```python
# config.py ä¸­çš„ ModelConfig
lora_r: int = 64          # LoRA rankï¼ˆè¶Šå¤§æ•ˆæœè¶Šå¥½ä½†å‚æ•°è¶Šå¤šï¼‰
lora_alpha: int = 128     # LoRA alphaï¼ˆé€šå¸¸ = r Ã— 2ï¼‰
lora_dropout: float = 0.1 # Dropoutç‡

# rank è°ƒæ•´å»ºè®®ï¼š
# r=16:  å¿«é€Ÿæµ‹è¯•ï¼Œæ˜¾å­˜å ç”¨å°‘
# r=64:  æ ‡å‡†é…ç½®ï¼ˆæ¨èï¼‰
# r=128: é«˜è´¨é‡è®­ç»ƒ
# r=256: æœ€ä½³æ•ˆæœï¼ˆéœ€è¦æ›´å¤šæ˜¾å­˜ï¼‰
```

### æ­¥éª¤ 4: å¼€å§‹è®­ç»ƒ

#### åŸºç¡€è®­ç»ƒå‘½ä»¤

```bash
# ä½¿ç”¨é»˜è®¤é…ç½®è®­ç»ƒ
python start.py train

# è‡ªå®šä¹‰å‚æ•°è®­ç»ƒ
python start.py train --epochs 5 --batch-size 2 --lr 1e-4

# åå°è®­ç»ƒï¼ˆæ¨èï¼‰
nohup python start.py train > logs/train.log 2>&1 &

# ç›‘æ§è®­ç»ƒè¿›åº¦
tail -f logs/train.log

# ä½¿ç”¨ TensorBoard å¯è§†åŒ–
tensorboard --logdir logs/tensorboard
```

#### ä» checkpoint æ¢å¤è®­ç»ƒ

```bash
# è®­ç»ƒä¸­æ–­åï¼Œä»æœ€æ–°çš„ checkpoint æ¢å¤
python start.py train --resume ./checkpoints/checkpoint-1000

# æˆ–æŒ‡å®šå…·ä½“ checkpoint
python start.py train --resume ./checkpoints/checkpoint-5000
```

### æ­¥éª¤ 5: ç›‘æ§è®­ç»ƒçŠ¶æ€

#### è®­ç»ƒè¾“å‡ºè§£è¯»

```
è®­ç»ƒé…ç½®
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ å‚æ•°           â”‚ å€¼                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ åŸºç¡€æ¨¡å‹       â”‚ Qwen/Qwen2.5-7B-Instruct â”‚
â”‚ è®­ç»ƒè½®æ•°       â”‚ 3                         â”‚
â”‚ æ‰¹æ¬¡å¤§å°       â”‚ 1                         â”‚
â”‚ æ¢¯åº¦ç´¯ç§¯       â”‚ 4                         â”‚
â”‚ å­¦ä¹ ç‡         â”‚ 0.0002                    â”‚
â”‚ æœ‰æ•ˆæ‰¹æ¬¡å¤§å°   â”‚ 4                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

{'loss': 2.8476, 'grad_norm': 1.234, 'learning_rate': 1.8e-5, 'epoch': 0.01}
  â†‘              â†‘            â†‘                  â†‘
  è®­ç»ƒæŸå¤±       æ¢¯åº¦èŒƒæ•°     å½“å‰å­¦ä¹ ç‡         å½“å‰è¿›åº¦

# æŒ‡æ ‡è¯´æ˜ï¼š
# - loss: è¶Šä½è¶Šå¥½ï¼Œåº”è¯¥ç¨³å®šä¸‹é™
# - grad_norm: æ¢¯åº¦èŒƒæ•°ï¼Œè¿‡å¤§å¯èƒ½éœ€è¦é™ä½å­¦ä¹ ç‡
# - learning_rate: å®é™…å­¦ä¹ ç‡ï¼ˆç»è¿‡warmupè°ƒæ•´ï¼‰
# - epoch: å·²å®Œæˆçš„è®­ç»ƒè½®æ•°
```

#### åˆ¤æ–­è®­ç»ƒæ˜¯å¦æ­£å¸¸

âœ… **è®­ç»ƒæ­£å¸¸çš„æ ‡å¿—**ï¼š
- Loss ç¨³å®šä¸‹é™
- æ¢¯åº¦èŒƒæ•° < 10
- ç”Ÿæˆçš„æ–‡æœ¬è´¨é‡é€æ¸æå‡
- éªŒè¯ Loss ä¸ä¸Šå‡

âŒ **è®­ç»ƒå¼‚å¸¸çš„æ ‡å¿—**ï¼š
- Loss éœ‡è¡æˆ– NaN
- æ¢¯åº¦èŒƒæ•°çˆ†ç‚¸ï¼ˆ>100ï¼‰
- éªŒè¯ Loss æŒç»­ä¸Šå‡ï¼ˆè¿‡æ‹Ÿåˆï¼‰
- æ˜¾å­˜æº¢å‡ºï¼ˆOOMï¼‰

### æ­¥éª¤ 6: ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹

```bash
# æ–¹å¼ä¸€ï¼šä½¿ç”¨ final_model
python start.py webui --lora ./checkpoints/final_model

# æ–¹å¼äºŒï¼šä½¿ç”¨æŒ‡å®š checkpoint
python start.py webui --lora ./checkpoints/checkpoint-5000

# å¯åŠ¨åè®¿é—® http://localhost:7860
```

### ä¸åŒåœºæ™¯çš„é…ç½®æ–¹æ¡ˆ

#### åœºæ™¯ 1: å¿«é€Ÿæµ‹è¯•ï¼ˆéªŒè¯æµç¨‹ï¼‰

```python
# config.py
max_seq_length: int = 1024
per_device_train_batch_size: int = 1
gradient_accumulation_steps: int = 2
num_train_epochs: int = 1
save_steps: int = 100
```

```bash
# ä½¿ç”¨å°‘é‡æ•°æ®æµ‹è¯•
python start.py prepare --sample
python start.py train --epochs 1
```

#### åœºæ™¯ 2: æ ‡å‡†è®­ç»ƒï¼ˆæ¨èé…ç½®ï¼‰

```python
# config.py
max_seq_length: int = 2048
per_device_train_batch_size: int = 1
gradient_accumulation_steps: int = 4
num_train_epochs: int = 3
learning_rate: float = 2e-4
save_steps: int = 500
```

#### åœºæ™¯ 3: é«˜è´¨é‡è®­ç»ƒï¼ˆå¤§æ•°æ®é‡ï¼‰

```python
# config.py
max_seq_length: int = 4096
per_device_train_batch_size: int = 2
gradient_accumulation_steps: int = 4
num_train_epochs: int = 2
learning_rate: float = 1e-4
lora_r: int = 128
lora_alpha: int = 256
save_steps: int = 1000
```

#### åœºæ™¯ 4: ä½æ˜¾å­˜ä¼˜åŒ–ï¼ˆ< 12GBï¼‰

```python
# config.py
load_in_4bit: bool = True
max_seq_length: int = 1024
per_device_train_batch_size: int = 1
gradient_accumulation_steps: int = 8
gradient_checkpointing: bool = True
```

## ğŸ“– ä½¿ç”¨æŒ‡å—

### WebUI åŠŸèƒ½

#### ğŸ“ åˆ›ä½œæ ‡ç­¾é¡µ
- è¾“å…¥åˆ›ä½œè¦æ±‚ï¼ŒAI è‡ªåŠ¨ç”Ÿæˆå°è¯´å†…å®¹
- æ”¯æŒè°ƒæ•´ç”Ÿæˆå‚æ•° (æ¸©åº¦ã€top-pã€top-k)
- å¯å¯ç”¨/ç¦ç”¨è®°å¿†åŠŸèƒ½

#### ğŸ§  è®°å¿†æ ‡ç­¾é¡µ
- **äººç‰©è®°å¿†**: æ·»åŠ å’Œç®¡ç†å°è¯´äººç‰©ä¿¡æ¯
- **æƒ…èŠ‚è®°å¿†**: è®°å½•æ•…äº‹æƒ…èŠ‚å‘å±•
- **ç¯å¢ƒè®¾å®š**: å­˜å‚¨åœºæ™¯å’Œä¸–ç•Œè§‚è®¾å®š
- **é‡è¦å¯¹è¯**: ä¿å­˜å…³é”®å¯¹è¯å†…å®¹

#### ğŸ¯ è®­ç»ƒæ ‡ç­¾é¡µ
- é…ç½®è®­ç»ƒå‚æ•°
- ç›‘æ§è®­ç»ƒè¿›åº¦

#### âš™ï¸ è®¾ç½®æ ‡ç­¾é¡µ
- æŸ¥çœ‹ç³»ç»ŸçŠ¶æ€
- é…ç½®æ¨¡å‹è·¯å¾„

### CLI å‘½ä»¤

```bash
# å¯åŠ¨ WebUI
python start.py webui [OPTIONS]

  OPTIONS:
    --lora PATH     LoRA æƒé‡è·¯å¾„
    --host ADDR     æœåŠ¡å™¨åœ°å€ (é»˜è®¤: 0.0.0.0)
    --port PORT     ç«¯å£ (é»˜è®¤: 7860)
    --share         åˆ›å»ºå…¬å…±é“¾æ¥

# å‡†å¤‡è®­ç»ƒæ•°æ®
python start.py prepare [OPTIONS]

  OPTIONS:
    --sample        åˆ›å»ºç¤ºä¾‹æ•°æ®
    --chunk-size    è®­ç»ƒå—å¤§å° (é»˜è®¤: 2048)
    --val-split     éªŒè¯é›†æ¯”ä¾‹ (é»˜è®¤: 0.1)

# å¯åŠ¨è®­ç»ƒ
python start.py train [OPTIONS]

  OPTIONS:
    --data PATH     è®­ç»ƒæ•°æ®è·¯å¾„
    --epochs N      è®­ç»ƒè½®æ•° (é»˜è®¤: 3)
    --batch-size N  æ‰¹æ¬¡å¤§å° (é»˜è®¤: 2)
    --lr FLOAT      å­¦ä¹ ç‡ (é»˜è®¤: 2e-4)
    --resume PATH   ä» checkpoint æ¢å¤

# æ¨ç†æµ‹è¯•
python start.py inference

# æ¨¡å‹æ ¼å¼è½¬æ¢
python start.py convert [SUBCOMMAND]

  SUBCOMMANDS:
    hf-to-gguf      è½¬æ¢ Hugging Face æ¨¡å‹ä¸º GGUF æ ¼å¼
    lora-to-gguf    è½¬æ¢ LoRA æƒé‡ä¸º GGUF æ ¼å¼
```

## ğŸ“ é¡¹ç›®ç»“æ„

```
novel_ai_system/
â”œâ”€â”€ config.py              # é…ç½®æ–‡ä»¶
â”œâ”€â”€ start.py               # ä¸»å…¥å£
â”œâ”€â”€ requirements.txt       # ä¾èµ–åˆ—è¡¨
â”œâ”€â”€ README.md             # è¯´æ˜æ–‡æ¡£
â”‚
â”œâ”€â”€ data/                  # æ•°æ®ç›®å½•
â”‚   â”œâ”€â”€ raw/              # åŸå§‹å°è¯´æ–‡ä»¶
â”‚   â”œâ”€â”€ train/            # è®­ç»ƒæ•°æ®
â”‚   â”œâ”€â”€ val/              # éªŒè¯æ•°æ®
â”‚   â””â”€â”€ chroma_db/        # å‘é‡æ•°æ®åº“
â”‚
â”œâ”€â”€ checkpoints/           # è®­ç»ƒæ£€æŸ¥ç‚¹
â”‚
â”œâ”€â”€ logs/                  # æ—¥å¿—æ–‡ä»¶
â”‚
â”œâ”€â”€ models/                # æ¨¡å‹æ–‡ä»¶ (GGUF)
â”‚
â”œâ”€â”€ scripts/               # è½¬æ¢è„šæœ¬
â”‚   â”œâ”€â”€ convert_hf_to_gguf.sh
â”‚   â””â”€â”€ convert_lora_to_gguf.sh
â”‚
â””â”€â”€ src/
    â”œâ”€â”€ train/            # è®­ç»ƒæ¨¡å—
    â”‚   â””â”€â”€ train_lora.py
    â”œâ”€â”€ inference/        # æ¨ç†æ¨¡å—
    â”‚   â”œâ”€â”€ backend_factory.py   # åç«¯å·¥å‚
    â”‚   â”œâ”€â”€ vllm_server.py       # vLLM æ¨ç†
    â”‚   â””â”€â”€ llama_server.py      # llama.cpp æ¨ç†
    â”œâ”€â”€ memory/           # è®°å¿†æ¨¡å—
    â”‚   â””â”€â”€ memory_manager.py
    â”œâ”€â”€ data/             # æ•°æ®å¤„ç†
    â”‚   â””â”€â”€ prepare_data.py
    â””â”€â”€ webui/            # Webç•Œé¢
        â””â”€â”€ app.py
```

## âš™ï¸ é…ç½®è¯´æ˜

ç¼–è¾‘ `config.py` è‡ªå®šä¹‰é…ç½®:

```python
# === æ¨ç†åç«¯é€‰æ‹© ===
model.inference_backend = "llama_cpp"  # "vllm" (GPU) æˆ– "llama_cpp" (CPU)

# === vLLM é…ç½® (GPU æ¨ç†) ===
model.vllm_max_model_len = 32768
model.vllm_gpu_memory_utilization = 0.85

# === llama.cpp é…ç½® (CPU æ¨ç†) ===
model.llama_cpp_model_path = "./models/qwen2.5-7b-q5_k_m.gguf"
model.llama_cpp_lora_path = "./models/lora-gguf"
model.llama_cpp_n_ctx = 32768       # ä¸Šä¸‹æ–‡é•¿åº¦
model.llama_cpp_n_threads = 6       # CPU çº¿ç¨‹æ•°

# === è®­ç»ƒé…ç½® ===
model.base_model = "Qwen/Qwen2.5-7B-Instruct"  # åŸºç¡€æ¨¡å‹ (Hugging Face æ ¼å¼)
model.load_in_4bit = True                       # 4-bit é‡åŒ–
model.lora_r = 64                               # LoRA rank

training.num_train_epochs = 3
training.per_device_train_batch_size = 2
training.gradient_accumulation_steps = 8
training.learning_rate = 2e-4

# === è®°å¿†é…ç½® ===
memory.embedding_model = "BAAI/bge-m3"
memory.max_memory_items = 1000
```

## ğŸ”§ æŠ€æœ¯æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Gradio WebUI                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚             â”‚             â”‚
        â–¼             â–¼             â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Inferenceâ”‚  â”‚ LoRA    â”‚  â”‚ ChromaDB  â”‚
   â”‚ Backend  â”‚  â”‚ å¾®è°ƒ    â”‚  â”‚ è®°å¿†å­˜å‚¨  â”‚
   â”‚  Factory â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â”‚         â”‚
    â–¼         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ vLLM  â”‚ â”‚llama.cpp â”‚
â”‚ (GPU) â”‚ â”‚  (CPU)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚         â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚   Qwen2.5   â”‚
  â”‚ (HF/GGUF)   â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### GPU è®­ç»ƒ + CPU æ¨ç†å·¥ä½œæµ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GPU æœºå™¨ - è®­ç»ƒé˜¶æ®µ                              â”‚
â”‚                                                                       â”‚
â”‚  Qwen/Qwen2.5-7B-Instruct (Hugging Face)                            â”‚
â”‚                â”‚                                                    â”‚
â”‚                â”œâ”€â–º LoRA è®­ç»ƒ â”€â”€â–º adapter_model.safetensors           â”‚
â”‚                                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼ è½¬æ¢
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æ¨¡å‹æ ¼å¼è½¬æ¢                                     â”‚
â”‚                                                                       â”‚
â”‚  HF æ¨¡å‹ â”€â”€convert_hf_to_ggufâ”€â”€â–º FP16 GGUF â”€â”€quantizeâ”€â”€â–º Q5 GGUF  â”‚
â”‚  LoRA â”€â”€â”€convert-lora-to-ggufâ”€â”€â–º GGUF LoRA adapter                 â”‚
â”‚                                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CPU æœºå™¨ - æ¨ç†é˜¶æ®µ                              â”‚
â”‚                                                                       â”‚
â”‚  llama.cpp åŠ è½½ GGUF æ¨¡å‹ + GGUF LoRA                               â”‚
â”‚                                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“š æ¨èåŸºç¡€æ¨¡å‹

- **Qwen2.5-7B-Instruct**: å¹³è¡¡æ€§èƒ½å’Œèµ„æºå ç”¨
- **Qwen2.5-14B-Instruct**: æ›´å¼ºæ€§èƒ½ï¼Œéœ€è¦æ›´å¤šæ˜¾å­˜
- **Yi-1.5-9B-Chat**: ä¼˜ç§€çš„ä¸­æ–‡å¯¹è¯æ¨¡å‹
- **DeepSeek-V3**: æœ€æ–°çš„å¼€æºä¸­æ–‡æ¨¡å‹

## ğŸ› å¸¸è§é—®é¢˜

### æ¨ç†åç«¯é€‰æ‹©
- **GPU æœºå™¨**: ä½¿ç”¨ `inference_backend = "vllm"` è·å¾—æœ€ä½³æ€§èƒ½
- **CPU æœºå™¨**: ä½¿ç”¨ `inference_backend = "llama_cpp"` è¿›è¡Œ CPU æ¨ç†
- **GGUF æ¨¡å‹è½¬æ¢**: ä½¿ç”¨ `python start.py convert hf-to-gguf` è½¬æ¢æ¨¡å‹

### GGUF æ¨¡å‹è½¬æ¢
```bash
# è½¬æ¢åŸºç¡€æ¨¡å‹ (ä¸€æ¬¡å³å¯)
python start.py convert hf-to-gguf --model Qwen/Qwen2.5-7B-Instruct --quant Q5_K_M

# è½¬æ¢ LoRA æƒé‡ (æ¯æ¬¡è®­ç»ƒå)
python start.py convert lora-to-gguf --lora-path ./checkpoints/final_model
```

### llama.cpp CPU æ¨ç†æ€§èƒ½
- **Q5_K_M é‡åŒ–**: ~4-6 tokens/ç§’ (6æ ¸ CPU)
- **Q8_0 é‡åŒ–**: ~3-5 tokens/ç§’ (æ›´é«˜ç²¾åº¦)
- è°ƒæ•´ `llama_cpp_n_threads` ä»¥åŒ¹é… CPU æ ¸å¿ƒæ•°

### æ˜¾å­˜ä¸è¶³
- ä½¿ç”¨ 4-bit é‡åŒ–: `model.load_in_4bit = True`
- å‡å°æ‰¹æ¬¡å¤§å°: `training.per_device_train_batch_size = 1`
- å‡å°æœ€å¤§åºåˆ—é•¿åº¦: `training.max_seq_length = 2048`

### è®­ç»ƒé€Ÿåº¦æ…¢
- å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹: `training.gradient_checkpointing = True`
- ä½¿ç”¨ DeepSpeed ä¼˜åŒ–å™¨
- å¢åŠ æ¢¯åº¦ç´¯ç§¯æ­¥æ•°

### ç”Ÿæˆè´¨é‡ä¸ä½³
- å¢åŠ è®­ç»ƒæ•°æ®é‡
- è°ƒæ•´æ¸©åº¦å‚æ•° (0.7-0.9)
- ä½¿ç”¨æ›´å¤§çš„åŸºç¡€æ¨¡å‹
- è®­ç»ƒæ›´å¤šè½®æ¬¡

## ğŸ” é—®é¢˜æ’æŸ¥ä¸è§£å†³æ–¹æ¡ˆ

### WebUI ç”Ÿæˆæ— å“åº”é—®é¢˜

#### é—®é¢˜ç°è±¡
- WebUI ç•Œé¢å¯æ­£å¸¸è®¿é—®
- ç‚¹å‡»ç”ŸæˆæŒ‰é’®åæ— ä»»ä½•è¾“å‡º
- debug.log æ˜¾ç¤ºè°ƒç”¨é“¾æ­£å¸¸ï¼Œä½† `async for` å¾ªç¯å¡ä½

#### æ ¹æœ¬åŸå› 
**äº‹ä»¶å¾ªç¯ä¸åŒ¹é…**ï¼šGradio çš„ async handler è¿è¡Œåœ¨å®ƒè‡ªå·±çš„äº‹ä»¶å¾ªç¯ä¸­ï¼Œè€Œ vLLM çš„ `AsyncLLMEngine` åœ¨ä¸»äº‹ä»¶å¾ªç¯ä¸­åˆå§‹åŒ–ã€‚å½“åœ¨ä¸åŒäº‹ä»¶å¾ªç¯é—´è°ƒç”¨æ—¶ï¼Œ`async for` æ— æ³•æ­£ç¡®è·å–å¼‚æ­¥ç”Ÿæˆå™¨çš„æ•°æ®ã€‚

#### è§£å†³æ–¹æ¡ˆ

**1. ä¿®æ”¹ `src/webui/app.py`**

```python
# å…¨å±€å˜é‡ä¿å­˜å¼•æ“çš„äº‹ä»¶å¾ªç¯
_engine_event_loop = None

async def launch_webui(lora_path: Optional[str] = None):
    global _engine_event_loop
    # ä¿å­˜å¼•æ“çš„äº‹ä»¶å¾ªç¯
    _engine_event_loop = asyncio.get_running_loop()

    # åˆå§‹åŒ–å¼•æ“...
    await _webui.initialize(lora_path)

    # æ„å»ºUI
    app = _webui.build_ui()

    # å…³é”®ï¼šprevent_thread_lock=True ä¸é˜»å¡äº‹ä»¶å¾ªç¯
    app.launch(
        prevent_thread_lock=True,  # å¿…é¡»è®¾ç½®
        server_name=config.webui.host,
        server_port=config.webui.port,
        ...
    )

    # ä¿æŒäº‹ä»¶å¾ªç¯æŒç»­è¿è¡Œ
    try:
        await asyncio.Future()  # æ— é™ç­‰å¾…
    except KeyboardInterrupt:
        print("\næ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨å…³é—­...")
```

**2. ä¿®æ”¹ `src/inference/vllm_server.py`**

```python
async def _generate(self, prompt: str, sampling_params: SamplingParams, ...):
    # è·å–å¼•æ“çš„äº‹ä»¶å¾ªç¯
    from src.webui.app import _engine_event_loop
    engine_loop = _engine_event_loop
    current_loop = asyncio.get_running_loop()

    # å®šä¹‰åœ¨å¼•æ“äº‹ä»¶å¾ªç¯ä¸­è¿è¡Œçš„åç¨‹
    async def generate_in_engine_loop():
        outputs = []
        async for request_output in self.engine.generate(...):
            outputs.append(request_output.outputs[0].text)
        return "".join(outputs)

    # æ£€æŸ¥æ˜¯å¦åœ¨åŒä¸€äº‹ä»¶å¾ªç¯
    if current_loop is engine_loop:
        return await generate_in_engine_loop()
    else:
        # è·¨äº‹ä»¶å¾ªç¯è°ƒç”¨
        future = asyncio.run_coroutine_threadsafe(
            generate_in_engine_loop(), engine_loop
        )
        return future.result(timeout=120)
```

#### å…³é”®è¦ç‚¹
1. **`prevent_thread_lock=True`**: è®© Gradio ä¸é˜»å¡ä¸»äº‹ä»¶å¾ªç¯
2. **`await asyncio.Future()`**: ä¿æŒä¸»äº‹ä»¶å¾ªç¯æŒç»­è¿è¡Œ
3. **`asyncio.run_coroutine_threadsafe()`**: è·¨äº‹ä»¶å¾ªç¯å®‰å…¨åœ°è°ƒç”¨å¼‚æ­¥å‡½æ•°
4. **ä¿å­˜å¼•æ“äº‹ä»¶å¾ªç¯**: åœ¨å…¨å±€å˜é‡ä¸­ä¿å­˜ `_engine_event_loop`

#### è°ƒè¯•æ—¥å¿—ç¤ºä¾‹
```
[vLLM._generate] å½“å‰äº‹ä»¶å¾ªç¯: 140427378303600
[vLLM._generate] å¼•æ“äº‹ä»¶å¾ªç¯: 140432959270800
[vLLM._generate] åœ¨ä¸åŒäº‹ä»¶å¾ªç¯ä¸­ï¼Œä½¿ç”¨ run_coroutine_threadsafe
[EngineLoop] æ”¶åˆ°chunk #1, æ–°å¢: 0, æ€»: 0
[EngineLoop] æ”¶åˆ°chunk #10, æ–°å¢: 1, æ€»: 2
...
[EngineLoop] ç”Ÿæˆå®Œæˆï¼Œå…± 287 å—ï¼Œé•¿åº¦: 417
```

### GPU å†…å­˜å ç”¨é—®é¢˜

#### ç°è±¡
- å¯åŠ¨æ—¶æç¤º GPU å†…å­˜ä¸è¶³
- `ValueError: Free memory on device is less than desired GPU memory utilization`

#### è§£å†³æ–¹æ¡ˆ
```bash
# æŸ¥æ‰¾å¹¶æ¸…ç†æ—§çš„ vLLM è¿›ç¨‹
ps aux | grep -E "vllm|VLLM" | grep $USER
kill -9 <PID>

# æˆ–ä½¿ç”¨ä¸€é”®æ¸…ç†
pkill -f "python3 start.py"
```

### vLLM å‚æ•°é—®é¢˜

#### é”™è¯¯ï¼šstop å‚æ•°åŒ…å«ç©ºå­—ç¬¦ä¸²
```
ValueError: stop cannot contain an empty string.
```

**è§£å†³æ–¹æ¡ˆ**ï¼šä¿®æ”¹ `src/inference/vllm_server.py`
```python
# é”™è¯¯å†™æ³•
stop=stop or ["<|im_end|>", ""]

# æ­£ç¡®å†™æ³•
stop=stop or ["<|im_end|>"]
```

### ä¾èµ–é—®é¢˜

#### torchaudio å®‰è£…å¤±è´¥
```bash
# ä¸å®‰è£… torchaudioï¼Œåªå®‰è£… torch
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121
```

#### Gradio 6.0+ API å˜åŒ–
- ç§»é™¤ `show_copy_button=True` å‚æ•°
- ç§»é™¤ `gr.Download` ç»„ä»¶ï¼Œä½¿ç”¨å­—ç¬¦ä¸²è¿”å›ä»£æ›¿

## ğŸ“ æ•°æ®æ ¼å¼

### TXT æ ¼å¼
```
ç›´æ¥æ”¾å…¥ data/raw/ ç›®å½•å³å¯
```

### JSON æ ¼å¼
```json
[
  {
    "title": "å°è¯´æ ‡é¢˜",
    "content": "å°è¯´å†…å®¹...",
    "author": "ä½œè€…",
    "genre": "ç±»å‹",
    "tags": ["æ ‡ç­¾1", "æ ‡ç­¾2"]
  }
]
```

### JSONL æ ¼å¼
```jsonl
{"title": "æ ‡é¢˜1", "content": "å†…å®¹1..."}
{"title": "æ ‡é¢˜2", "content": "å†…å®¹2..."}
```

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼

## ğŸ“„ è®¸å¯è¯

MIT License

## ğŸ™ è‡´è°¢

- [Qwen](https://github.com/QwenLM/Qwen) - ä¼˜ç§€çš„ä¸­æ–‡å¼€æºæ¨¡å‹
- [vLLM](https://github.com/vllm-project/vllm) - é«˜æ€§èƒ½ GPU æ¨ç†å¼•æ“
- [llama.cpp](https://github.com/ggml-org/llama.cpp) - é«˜æ•ˆ CPU æ¨ç†å¼•æ“
- [Gradio](https://github.com/gradio-app/gradio) - WebUI æ¡†æ¶
- [ChromaDB](https://github.com/chroma-core/chroma) - å‘é‡æ•°æ®åº“
- [PEFT](https://github.com/huggingface/peft) - LoRA å¾®è°ƒåº“

---

**ğŸš€ å¼€å§‹åˆ›ä½œä½ çš„å°è¯´ä¹‹æ—…ï¼**
