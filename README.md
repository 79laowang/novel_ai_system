# ä¸­æ–‡å°è¯´å†™ä½œAIç³»ç»Ÿ

åŸºäº **vLLM + LoRA + ChromaDB** çš„æ™ºèƒ½å°è¯´åˆ›ä½œç³»ç»Ÿï¼Œæ”¯æŒè®­ç»ƒã€æ¨ç†å’Œè®°å¿†ç®¡ç†ã€‚

## âœ¨ ç‰¹æ€§

- **é«˜æ€§èƒ½æ¨ç†**: åŸºäº vLLMï¼Œæ”¯æŒå¿«é€Ÿæ–‡æœ¬ç”Ÿæˆ
- **LoRA å¾®è°ƒ**: QLoRA 4-bit é‡åŒ–è®­ç»ƒï¼Œé™ä½æ˜¾å­˜éœ€æ±‚
- **è®°å¿†åŠŸèƒ½**: å‘é‡æ•°æ®åº“å­˜å‚¨ï¼Œæ”¯æŒé•¿æœŸè®°å¿†å’Œ RAG
- **WebUI ç•Œé¢**: Gradio æ„å»ºï¼Œæ˜“äºä½¿ç”¨
- **ä¸­æ–‡ä¼˜åŒ–**: ä¸“ä¸ºä¸­æ–‡å°è¯´å†™ä½œä¼˜åŒ–

## ğŸ“‹ ç³»ç»Ÿè¦æ±‚

- Python 3.8+
- CUDA 12.0+
- GPU: å»ºè®® 16GB+ æ˜¾å­˜ (RTX 4090 / A100 ç­‰)
- å†…å­˜: å»ºè®® 64GB+

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
cd novel_ai_system
pip install -r requirements.txt
```

### 2. å‡†å¤‡è®­ç»ƒæ•°æ®

åˆ›å»ºç¤ºä¾‹æ•°æ®ï¼ˆç”¨äºæµ‹è¯•ï¼‰:
```bash
python start.py prepare --sample
```

æˆ–ä½¿ç”¨è‡ªå·±çš„å°è¯´æ•°æ®:
```bash
# å°†å°è¯´æ–‡ä»¶æ”¾å…¥ data/raw/ ç›®å½•
# æ”¯æŒ .txt, .json, .jsonl æ ¼å¼
python start.py prepare
```

### 3. å¯åŠ¨ WebUI

```bash
python start.py webui
```

è®¿é—® `http://localhost:7860` å¼€å§‹ä½¿ç”¨ï¼

### 4. è®­ç»ƒæ¨¡å‹ (å¯é€‰)

```bash
python start.py train
```

è®­ç»ƒå®Œæˆåï¼Œä½¿ç”¨ LoRA æƒé‡å¯åŠ¨:
```bash
python start.py webui --lora ./checkpoints/final_model
```

## ğŸ“– ä½¿ç”¨æŒ‡å—

### WebUI åŠŸèƒ½

#### ğŸ“ åˆ›ä½œæ ‡ç­¾é¡µ
- è¾“å…¥åˆ›ä½œè¦æ±‚ï¼ŒAI è‡ªåŠ¨ç”Ÿæˆå°è¯´å†…å®¹
- æ”¯æŒè°ƒæ•´ç”Ÿæˆå‚æ•° (æ¸©åº¦ã€top-pã€top-k)
- å¯å¯ç”¨/ç¦ç”¨è®°å¿†åŠŸèƒ½

#### ğŸ§  è®°å¿†æ ‡ç­¾é¡µ
- **äººç‰©è®°å¿†**: æ·»åŠ å’Œç®¡ç†å°è¯´äººç‰©ä¿¡æ¯
- **æƒ…èŠ‚è®°å¿†**: è®°å½•æ•…äº‹æƒ…èŠ‚å‘å±•
- **ç¯å¢ƒè®¾å®š**: å­˜å‚¨åœºæ™¯å’Œä¸–ç•Œè§‚è®¾å®š
- **é‡è¦å¯¹è¯**: ä¿å­˜å…³é”®å¯¹è¯å†…å®¹

#### ğŸ¯ è®­ç»ƒæ ‡ç­¾é¡µ
- é…ç½®è®­ç»ƒå‚æ•°
- ç›‘æ§è®­ç»ƒè¿›åº¦

#### âš™ï¸ è®¾ç½®æ ‡ç­¾é¡µ
- æŸ¥çœ‹ç³»ç»ŸçŠ¶æ€
- é…ç½®æ¨¡å‹è·¯å¾„

### CLI å‘½ä»¤

```bash
# å¯åŠ¨ WebUI
python start.py webui [OPTIONS]

  OPTIONS:
    --lora PATH     LoRA æƒé‡è·¯å¾„
    --host ADDR     æœåŠ¡å™¨åœ°å€ (é»˜è®¤: 0.0.0.0)
    --port PORT     ç«¯å£ (é»˜è®¤: 7860)
    --share         åˆ›å»ºå…¬å…±é“¾æ¥

# å‡†å¤‡è®­ç»ƒæ•°æ®
python start.py prepare [OPTIONS]

  OPTIONS:
    --sample        åˆ›å»ºç¤ºä¾‹æ•°æ®
    --chunk-size    è®­ç»ƒå—å¤§å° (é»˜è®¤: 2048)
    --val-split     éªŒè¯é›†æ¯”ä¾‹ (é»˜è®¤: 0.1)

# å¯åŠ¨è®­ç»ƒ
python start.py train [OPTIONS]

  OPTIONS:
    --data PATH     è®­ç»ƒæ•°æ®è·¯å¾„
    --epochs N      è®­ç»ƒè½®æ•° (é»˜è®¤: 3)
    --batch-size N  æ‰¹æ¬¡å¤§å° (é»˜è®¤: 2)
    --lr FLOAT      å­¦ä¹ ç‡ (é»˜è®¤: 2e-4)

# æ¨ç†æµ‹è¯•
python start.py inference
```

## ğŸ“ é¡¹ç›®ç»“æ„

```
novel_ai_system/
â”œâ”€â”€ config.py              # é…ç½®æ–‡ä»¶
â”œâ”€â”€ start.py               # ä¸»å…¥å£
â”œâ”€â”€ requirements.txt       # ä¾èµ–åˆ—è¡¨
â”œâ”€â”€ README.md             # è¯´æ˜æ–‡æ¡£
â”‚
â”œâ”€â”€ data/                  # æ•°æ®ç›®å½•
â”‚   â”œâ”€â”€ raw/              # åŸå§‹å°è¯´æ–‡ä»¶
â”‚   â”œâ”€â”€ train/            # è®­ç»ƒæ•°æ®
â”‚   â”œâ”€â”€ val/              # éªŒè¯æ•°æ®
â”‚   â””â”€â”€ chroma_db/        # å‘é‡æ•°æ®åº“
â”‚
â”œâ”€â”€ checkpoints/           # è®­ç»ƒæ£€æŸ¥ç‚¹
â”‚
â”œâ”€â”€ logs/                  # æ—¥å¿—æ–‡ä»¶
â”‚
â”œâ”€â”€ models/                # ä¸‹è½½çš„æ¨¡å‹
â”‚
â””â”€â”€ src/
    â”œâ”€â”€ train/            # è®­ç»ƒæ¨¡å—
    â”‚   â””â”€â”€ train_lora.py
    â”œâ”€â”€ inference/        # æ¨ç†æ¨¡å—
    â”‚   â””â”€â”€ vllm_server.py
    â”œâ”€â”€ memory/           # è®°å¿†æ¨¡å—
    â”‚   â””â”€â”€ memory_manager.py
    â”œâ”€â”€ data/             # æ•°æ®å¤„ç†
    â”‚   â””â”€â”€ prepare_data.py
    â””â”€â”€ webui/            # Webç•Œé¢
        â””â”€â”€ app.py
```

## âš™ï¸ é…ç½®è¯´æ˜

ç¼–è¾‘ `config.py` è‡ªå®šä¹‰é…ç½®:

```python
# æ¨¡å‹é…ç½®
model.base_model = "Qwen/Qwen2.5-7B-Instruct"  # åŸºç¡€æ¨¡å‹
model.load_in_4bit = True                       # 4-bit é‡åŒ–
model.lora_r = 64                               # LoRA rank

# è®­ç»ƒé…ç½®
training.num_train_epochs = 3
training.per_device_train_batch_size = 2
training.gradient_accumulation_steps = 8
training.learning_rate = 2e-4

# æ¨ç†é…ç½®
model.vllm_max_model_len = 32768
model.vllm_gpu_memory_utilization = 0.85

# è®°å¿†é…ç½®
memory.embedding_model = "BAAI/bge-m3"
memory.max_memory_items = 1000
```

## ğŸ”§ æŠ€æœ¯æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Gradio WebUI                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚             â”‚             â”‚
        â–¼             â–¼             â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ vLLM    â”‚  â”‚ LoRA    â”‚  â”‚ ChromaDB  â”‚
   â”‚ æ¨ç†å¼•æ“ â”‚  â”‚ å¾®è°ƒ    â”‚  â”‚ è®°å¿†å­˜å‚¨  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚             â”‚
        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
               â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Qwen2.5   â”‚
        â”‚  (æˆ–å…¶ä»–)   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“š æ¨èåŸºç¡€æ¨¡å‹

- **Qwen2.5-7B-Instruct**: å¹³è¡¡æ€§èƒ½å’Œèµ„æºå ç”¨
- **Qwen2.5-14B-Instruct**: æ›´å¼ºæ€§èƒ½ï¼Œéœ€è¦æ›´å¤šæ˜¾å­˜
- **Yi-1.5-9B-Chat**: ä¼˜ç§€çš„ä¸­æ–‡å¯¹è¯æ¨¡å‹
- **DeepSeek-V3**: æœ€æ–°çš„å¼€æºä¸­æ–‡æ¨¡å‹

## ğŸ› å¸¸è§é—®é¢˜

### æ˜¾å­˜ä¸è¶³
- ä½¿ç”¨ 4-bit é‡åŒ–: `model.load_in_4bit = True`
- å‡å°æ‰¹æ¬¡å¤§å°: `training.per_device_train_batch_size = 1`
- å‡å°æœ€å¤§åºåˆ—é•¿åº¦: `training.max_seq_length = 2048`

### è®­ç»ƒé€Ÿåº¦æ…¢
- å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹: `training.gradient_checkpointing = True`
- ä½¿ç”¨ DeepSpeed ä¼˜åŒ–å™¨
- å¢åŠ æ¢¯åº¦ç´¯ç§¯æ­¥æ•°

### ç”Ÿæˆè´¨é‡ä¸ä½³
- å¢åŠ è®­ç»ƒæ•°æ®é‡
- è°ƒæ•´æ¸©åº¦å‚æ•° (0.7-0.9)
- ä½¿ç”¨æ›´å¤§çš„åŸºç¡€æ¨¡å‹
- è®­ç»ƒæ›´å¤šè½®æ¬¡

## ğŸ” é—®é¢˜æ’æŸ¥ä¸è§£å†³æ–¹æ¡ˆ

### WebUI ç”Ÿæˆæ— å“åº”é—®é¢˜

#### é—®é¢˜ç°è±¡
- WebUI ç•Œé¢å¯æ­£å¸¸è®¿é—®
- ç‚¹å‡»ç”ŸæˆæŒ‰é’®åæ— ä»»ä½•è¾“å‡º
- debug.log æ˜¾ç¤ºè°ƒç”¨é“¾æ­£å¸¸ï¼Œä½† `async for` å¾ªç¯å¡ä½

#### æ ¹æœ¬åŸå› 
**äº‹ä»¶å¾ªç¯ä¸åŒ¹é…**ï¼šGradio çš„ async handler è¿è¡Œåœ¨å®ƒè‡ªå·±çš„äº‹ä»¶å¾ªç¯ä¸­ï¼Œè€Œ vLLM çš„ `AsyncLLMEngine` åœ¨ä¸»äº‹ä»¶å¾ªç¯ä¸­åˆå§‹åŒ–ã€‚å½“åœ¨ä¸åŒäº‹ä»¶å¾ªç¯é—´è°ƒç”¨æ—¶ï¼Œ`async for` æ— æ³•æ­£ç¡®è·å–å¼‚æ­¥ç”Ÿæˆå™¨çš„æ•°æ®ã€‚

#### è§£å†³æ–¹æ¡ˆ

**1. ä¿®æ”¹ `src/webui/app.py`**

```python
# å…¨å±€å˜é‡ä¿å­˜å¼•æ“çš„äº‹ä»¶å¾ªç¯
_engine_event_loop = None

async def launch_webui(lora_path: Optional[str] = None):
    global _engine_event_loop
    # ä¿å­˜å¼•æ“çš„äº‹ä»¶å¾ªç¯
    _engine_event_loop = asyncio.get_running_loop()

    # åˆå§‹åŒ–å¼•æ“...
    await _webui.initialize(lora_path)

    # æ„å»ºUI
    app = _webui.build_ui()

    # å…³é”®ï¼šprevent_thread_lock=True ä¸é˜»å¡äº‹ä»¶å¾ªç¯
    app.launch(
        prevent_thread_lock=True,  # å¿…é¡»è®¾ç½®
        server_name=config.webui.host,
        server_port=config.webui.port,
        ...
    )

    # ä¿æŒäº‹ä»¶å¾ªç¯æŒç»­è¿è¡Œ
    try:
        await asyncio.Future()  # æ— é™ç­‰å¾…
    except KeyboardInterrupt:
        print("\næ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨å…³é—­...")
```

**2. ä¿®æ”¹ `src/inference/vllm_server.py`**

```python
async def _generate(self, prompt: str, sampling_params: SamplingParams, ...):
    # è·å–å¼•æ“çš„äº‹ä»¶å¾ªç¯
    from src.webui.app import _engine_event_loop
    engine_loop = _engine_event_loop
    current_loop = asyncio.get_running_loop()

    # å®šä¹‰åœ¨å¼•æ“äº‹ä»¶å¾ªç¯ä¸­è¿è¡Œçš„åç¨‹
    async def generate_in_engine_loop():
        outputs = []
        async for request_output in self.engine.generate(...):
            outputs.append(request_output.outputs[0].text)
        return "".join(outputs)

    # æ£€æŸ¥æ˜¯å¦åœ¨åŒä¸€äº‹ä»¶å¾ªç¯
    if current_loop is engine_loop:
        return await generate_in_engine_loop()
    else:
        # è·¨äº‹ä»¶å¾ªç¯è°ƒç”¨
        future = asyncio.run_coroutine_threadsafe(
            generate_in_engine_loop(), engine_loop
        )
        return future.result(timeout=120)
```

#### å…³é”®è¦ç‚¹
1. **`prevent_thread_lock=True`**: è®© Gradio ä¸é˜»å¡ä¸»äº‹ä»¶å¾ªç¯
2. **`await asyncio.Future()`**: ä¿æŒä¸»äº‹ä»¶å¾ªç¯æŒç»­è¿è¡Œ
3. **`asyncio.run_coroutine_threadsafe()`**: è·¨äº‹ä»¶å¾ªç¯å®‰å…¨åœ°è°ƒç”¨å¼‚æ­¥å‡½æ•°
4. **ä¿å­˜å¼•æ“äº‹ä»¶å¾ªç¯**: åœ¨å…¨å±€å˜é‡ä¸­ä¿å­˜ `_engine_event_loop`

#### è°ƒè¯•æ—¥å¿—ç¤ºä¾‹
```
[vLLM._generate] å½“å‰äº‹ä»¶å¾ªç¯: 140427378303600
[vLLM._generate] å¼•æ“äº‹ä»¶å¾ªç¯: 140432959270800
[vLLM._generate] åœ¨ä¸åŒäº‹ä»¶å¾ªç¯ä¸­ï¼Œä½¿ç”¨ run_coroutine_threadsafe
[EngineLoop] æ”¶åˆ°chunk #1, æ–°å¢: 0, æ€»: 0
[EngineLoop] æ”¶åˆ°chunk #10, æ–°å¢: 1, æ€»: 2
...
[EngineLoop] ç”Ÿæˆå®Œæˆï¼Œå…± 287 å—ï¼Œé•¿åº¦: 417
```

### GPU å†…å­˜å ç”¨é—®é¢˜

#### ç°è±¡
- å¯åŠ¨æ—¶æç¤º GPU å†…å­˜ä¸è¶³
- `ValueError: Free memory on device is less than desired GPU memory utilization`

#### è§£å†³æ–¹æ¡ˆ
```bash
# æŸ¥æ‰¾å¹¶æ¸…ç†æ—§çš„ vLLM è¿›ç¨‹
ps aux | grep -E "vllm|VLLM" | grep $USER
kill -9 <PID>

# æˆ–ä½¿ç”¨ä¸€é”®æ¸…ç†
pkill -f "python3 start.py"
```

### vLLM å‚æ•°é—®é¢˜

#### é”™è¯¯ï¼šstop å‚æ•°åŒ…å«ç©ºå­—ç¬¦ä¸²
```
ValueError: stop cannot contain an empty string.
```

**è§£å†³æ–¹æ¡ˆ**ï¼šä¿®æ”¹ `src/inference/vllm_server.py`
```python
# é”™è¯¯å†™æ³•
stop=stop or ["<|im_end|>", ""]

# æ­£ç¡®å†™æ³•
stop=stop or ["<|im_end|>"]
```

### ä¾èµ–é—®é¢˜

#### torchaudio å®‰è£…å¤±è´¥
```bash
# ä¸å®‰è£… torchaudioï¼Œåªå®‰è£… torch
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121
```

#### Gradio 6.0+ API å˜åŒ–
- ç§»é™¤ `show_copy_button=True` å‚æ•°
- ç§»é™¤ `gr.Download` ç»„ä»¶ï¼Œä½¿ç”¨å­—ç¬¦ä¸²è¿”å›ä»£æ›¿

## ğŸ“ æ•°æ®æ ¼å¼

### TXT æ ¼å¼
```
ç›´æ¥æ”¾å…¥ data/raw/ ç›®å½•å³å¯
```

### JSON æ ¼å¼
```json
[
  {
    "title": "å°è¯´æ ‡é¢˜",
    "content": "å°è¯´å†…å®¹...",
    "author": "ä½œè€…",
    "genre": "ç±»å‹",
    "tags": ["æ ‡ç­¾1", "æ ‡ç­¾2"]
  }
]
```

### JSONL æ ¼å¼
```jsonl
{"title": "æ ‡é¢˜1", "content": "å†…å®¹1..."}
{"title": "æ ‡é¢˜2", "content": "å†…å®¹2..."}
```

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼

## ğŸ“„ è®¸å¯è¯

MIT License

## ğŸ™ è‡´è°¢

- [Qwen](https://github.com/QwenLM/Qwen) - ä¼˜ç§€çš„ä¸­æ–‡å¼€æºæ¨¡å‹
- [vLLM](https://github.com/vllm-project/vllm) - é«˜æ€§èƒ½æ¨ç†å¼•æ“
- [Gradio](https://github.com/gradio-app/gradio) - WebUI æ¡†æ¶
- [ChromaDB](https://github.com/chroma-core/chroma) - å‘é‡æ•°æ®åº“

---

**ğŸš€ å¼€å§‹åˆ›ä½œä½ çš„å°è¯´ä¹‹æ—…ï¼**
