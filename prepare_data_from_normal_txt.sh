#!/bin/bash
# é€šç”¨æ•°æ®å‡†å¤‡è„šæœ¬ï¼šä»æŒ‡å®šç›®å½•å¤åˆ¶ TXT/JSON/JSONL æ–‡ä»¶å¹¶å‡†å¤‡è®­ç»ƒ
# ç”¨æ³•: ./prepare_data_from_normal_txt.sh <æºç›®å½•> [è¾“å‡ºåç§°]
#
# è¯´æ˜: è¾“å‡ºåç§°å¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨æºç›®å½•çš„ basename
#
# ç¤ºä¾‹:
#   ./prepare_data_from_normal_txt.sh ~/work/urban-novels
#   ./prepare_data_from_normal_txt.sh ~/work/wuxia-data wuxia

set -e

# é¢œè‰²è¾“å‡º
GREEN='\033[0;32m'
CYAN='\033[0;36m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
NC='\033[0m'

# ============================================
# å‚æ•°è§£æ
# ============================================
SOURCE_DIR="${1:-}"
OUTPUT_NAME="${2:-}"

# æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
show_help() {
    echo -e "${CYAN}========================================${NC}"
    echo -e "${CYAN}  é€šç”¨æ•°æ®å‡†å¤‡è„šæœ¬${NC}"
    echo -e "${CYAN}========================================${NC}"
    echo ""
    echo "ç”¨æ³•: $0 <æºç›®å½•> [è¾“å‡ºåç§°]"
    echo ""
    echo "å‚æ•°:"
    echo "  æºç›®å½•    - åŒ…å« TXT/JSON/JSONL æ–‡ä»¶çš„ç›®å½•è·¯å¾„"
    echo "  è¾“å‡ºåç§°  - æ•°æ®é›†åç§° (å¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨æºç›®å½•å)"
    echo ""
    echo "ç¤ºä¾‹:"
    echo "  $0 ~/work/urban-novels"
    echo "  $0 ~/work/wuxia-data wuxia"
    echo "  $0 ~/data/fantasy-novels fantasy"
    echo ""
    echo "ç”Ÿæˆçš„ç›®å½•ç»“æ„:"
    echo "  ./data/raw_<è¾“å‡ºåç§°>/     - åŸå§‹æ•°æ®"
    echo "  ./data/train_<è¾“å‡ºåç§°>/   - è®­ç»ƒæ•°æ®"
    echo "  ./data/val_<è¾“å‡ºåç§°>/     - éªŒè¯æ•°æ®"
    echo ""
    echo "é»˜è®¤å‘½åè§„åˆ™:"
    echo "  æºç›®å½• ~/work/urban-novels -> è¾“å‡ºå urban-novels"
    echo "  æºç›®å½• ~/data/xianxia_data -> è¾“å‡ºå xianxia_data"
    exit 0
}

# æ£€æŸ¥å‚æ•°
if [ -z "$SOURCE_DIR" ]; then
    echo -e "${RED}é”™è¯¯: ç¼ºå°‘æºç›®å½•å‚æ•°${NC}"
    echo ""
    show_help
fi

# å¦‚æœæ²¡æœ‰æŒ‡å®šè¾“å‡ºåç§°ï¼Œä»æºç›®å½•æ¨å¯¼
if [ -z "$OUTPUT_NAME" ]; then
    # è·å–æºç›®å½•çš„ basenameï¼Œå¹¶ç§»é™¤å¯èƒ½çš„åç¼€å¦‚ -data, _raw ç­‰
    DIR_NAME=$(basename "$SOURCE_DIR")

    # æ¸…ç†ç›®å½•åä¸­çš„å¸¸è§åç¼€
    OUTPUT_NAME=$(echo "$DIR_NAME" | sed -E 's/[-_]?(data|raw|files|novels|txt)?$//g' | sed 's/^-+//;s/-+$//')

    # å¦‚æœæ¸…ç†åä¸ºç©ºï¼Œä½¿ç”¨åŸç›®å½•å
    if [ -z "$OUTPUT_NAME" ]; then
        OUTPUT_NAME="$DIR_NAME"
    fi

    echo -e "${YELLOW}è¾“å‡ºåç§°æœªæŒ‡å®šï¼Œä½¿ç”¨æ¨å¯¼å€¼: $OUTPUT_NAME${NC}"
fi

# æ‰©å±• ~ ç¬¦å·
SOURCE_DIR="${SOURCE_DIR/#\~/$HOME}"

# ============================================
# é…ç½®å‚æ•° (å¯æ‰‹åŠ¨ä¿®æ”¹)
# ============================================
RAW_DIR="./data/raw_${OUTPUT_NAME}"         # åŸå§‹æ•°æ®ç›®å½•
TRAIN_DIR="./data/train_${OUTPUT_NAME}"     # è®­ç»ƒæ•°æ®ç›®å½•
VAL_DIR="./data/val_${OUTPUT_NAME}"         # éªŒè¯æ•°æ®ç›®å½•

CHUNK_SIZE=2048                        # è®­ç»ƒå—å¤§å°
VAL_SPLIT=0.1                          # éªŒè¯é›†æ¯”ä¾‹ (10%)
MIN_LENGTH=500                         # æœ€å°æ–‡æœ¬é•¿åº¦

# ============================================
# å‡½æ•°å®šä¹‰
# ============================================

print_header() {
    echo -e "${CYAN}========================================${NC}"
    echo -e "${CYAN}  é€šç”¨æ•°æ®å‡†å¤‡è„šæœ¬${NC}"
    echo -e "${CYAN}========================================${NC}"
}

print_step() {
    echo -e "\n${CYAN}[$1] $2${NC}"
}

print_success() {
    echo -e "${GREEN}âœ“ $1${NC}"
}

print_error() {
    echo -e "${RED}âœ— $1${NC}"
}

# ============================================
# ä¸»æµç¨‹
# ============================================

print_header
echo -e "${CYAN}æºç›®å½•: ${SOURCE_DIR}${NC}"
echo -e "${CYAN}è¾“å‡ºåç§°: ${OUTPUT_NAME}${NC}"
echo -e "${CYAN}è¾“å‡ºç›®å½•: ${RAW_DIR}${NC}"

# 1. æ£€æŸ¥æºæ•°æ®
print_step "1/5" "æ£€æŸ¥æºæ•°æ®ç›®å½•..."
if [ ! -d "$SOURCE_DIR" ]; then
    print_error "æºç›®å½•ä¸å­˜åœ¨: $SOURCE_DIR"
    exit 1
fi

FILE_COUNT=$(find "$SOURCE_DIR" -type f \( -name "*.txt" -o -name "*.json" -o -name "*.jsonl" \) 2>/dev/null | wc -l)
if [ "$FILE_COUNT" -eq 0 ]; then
    print_error "åœ¨ $SOURCE_DIR ä¸­æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶"
    exit 1
fi
print_success "æ‰¾åˆ° $FILE_COUNT ä¸ªæ•°æ®æ–‡ä»¶"

# æ˜¾ç¤ºæ–‡ä»¶åˆ—è¡¨
echo -e "\n${YELLOW}æ–‡ä»¶åˆ—è¡¨:${NC}"
ls -lh "$SOURCE_DIR/"

# 2. æ¸…ç†å¹¶åˆ›å»ºç›®å½•
print_step "2/5" "å‡†å¤‡æ•°æ®ç›®å½•..."

# æ¸…ç©ºæ—§æ•°æ®
rm -rf "$RAW_DIR" "$TRAIN_DIR" "$VAL_DIR"

# åˆ›å»ºæ–°ç›®å½•
mkdir -p "$RAW_DIR"
mkdir -p "$TRAIN_DIR"
mkdir -p "$VAL_DIR"

print_success "ç›®å½•å·²åˆ›å»º"

# 3. å¤åˆ¶æ•°æ®æ–‡ä»¶
print_step "3/5" "å¤åˆ¶æ•°æ®æ–‡ä»¶..."

cp -r "$SOURCE_DIR"/* "$RAW_DIR/" 2>/dev/null || true

COPIED_COUNT=$(find "$RAW_DIR" -type f | wc -l)
print_success "å·²å¤åˆ¶ $COPIED_COUNT ä¸ªæ–‡ä»¶åˆ° $RAW_DIR"

# 4. å‡†å¤‡è®­ç»ƒæ•°æ® (ä½¿ç”¨ Python)
print_step "4/5" "ç”Ÿæˆè®­ç»ƒæ•°æ®..."

cat > /tmp/prepare_data_general.py << PYTHON_EOF
#!/usr/bin/env python3
import os
import sys
import json
import re
from pathlib import Path

# é…ç½®
RAW_DIR = "./data/raw_${OUTPUT_NAME}"
TRAIN_DIR = "./data/train_${OUTPUT_NAME}"
VAL_DIR = "./data/val_${OUTPUT_NAME}"

CHUNK_SIZE = ${CHUNK_SIZE}
VAL_SPLIT = ${VAL_SPLIT}
MIN_LENGTH = ${MIN_LENGTH}
OVERLAP = 200

def clean_text(text: str) -> str:
    """æ¸…ç†æ–‡æœ¬"""
    # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
    text = re.sub(r'[\x00-\x08\x0b-\x0c\x0e-\x1f\x7f-\x9f]', '', text)
    # æ ‡å‡†åŒ–ç©ºç™½
    text = re.sub(r'\s+', ' ', text)
    # ç§»é™¤è¿‡å¤šçš„æ¢è¡Œ
    text = re.sub(r'\n{3,}', '\n\n', text)
    return text.strip()

def split_into_chunks(text: str, chunk_size: int = CHUNK_SIZE, overlap: int = OVERLAP):
    """å°†æ–‡æœ¬åˆ†å‰²æˆè®­ç»ƒå—"""
    chunks = []
    start = 0
    text_len = len(text)

    while start < text_len:
        end = start + chunk_size
        chunk = text[start:end]

        # å°è¯•åœ¨å¥å­è¾¹ç•Œåˆ†å‰²
        if end < text_len:
            last_period = chunk.rfind('ã€‚')
            last_newline = chunk.rfind('\n')
            split_pos = max(last_period, last_newline)

            if split_pos > chunk_size * 0.7:  # è‡³å°‘ä¿ç•™70%
                chunk = text[start:start + split_pos + 1]
                end = start + split_pos + 1

        chunks.append(chunk.strip())
        start = end - overlap if end < text_len else end

    return [c for c in chunks if len(c) >= MIN_LENGTH]

def load_txt_file(file_path: Path) -> dict:
    """åŠ è½½ TXT æ–‡ä»¶"""
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()

    content = clean_text(content)

    return {
        "title": file_path.stem,
        "content": content,
        "source": str(file_path)
    }

def main():
    raw_dir = Path(RAW_DIR)
    train_dir = Path(TRAIN_DIR)
    val_dir = Path(VAL_DIR)

    print(f"ğŸ“ æ‰«æç›®å½•: {raw_dir}")

    # æŸ¥æ‰¾æ‰€æœ‰æ–‡æœ¬æ–‡ä»¶
    txt_files = list(raw_dir.glob("*.txt"))
    json_files = list(raw_dir.glob("*.json"))
    jsonl_files = list(raw_dir.glob("*.jsonl"))

    all_files = txt_files + json_files + jsonl_files

    if not all_files:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•æ•°æ®æ–‡ä»¶!")
        return

    print(f"âœ“ æ‰¾åˆ° {len(all_files)} ä¸ªæ–‡ä»¶")

    # åŠ è½½æ‰€æœ‰æ•°æ®
    all_novels = []
    all_chunks = []

    for file_path in all_files:
        try:
            if file_path.suffix == '.txt':
                novel = load_txt_file(file_path)
                all_novels.append(novel)

                # åˆ†å—
                chunks = split_into_chunks(novel['content'])
                for chunk in chunks:
                    all_chunks.append({
                        "text": chunk,
                        "title": novel['title'],
                        "source": novel['source']
                    })

                print(f"  âœ“ {file_path.name} -> {len(chunks)} å—")

            elif file_path.suffix == '.json':
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    # å¤„ç† JSON æ•°æ®...
                    print(f"  âš  JSON æ–‡ä»¶: {file_path.name} (éœ€æ‰‹åŠ¨å¤„ç†)")

            elif file_path.suffix == '.jsonl':
                with open(file_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        if line.strip():
                            data = json.loads(line)
                            text = data.get('text', data.get('content', ''))
                            if text:
                                chunks = split_into_chunks(clean_text(text))
                                for chunk in chunks:
                                    all_chunks.append({
                                        "text": chunk,
                                        "title": data.get('title', file_path.stem),
                                        "source": str(file_path)
                                    })
                    print(f"  âœ“ {file_path.name} -> JSONL å¤„ç†å®Œæˆ")

        except Exception as e:
            print(f"  âœ— {file_path.name}: {e}")

    if not all_chunks:
        print("âŒ æ²¡æœ‰ç”Ÿæˆä»»ä½•è®­ç»ƒå—!")
        return

    print(f"\nğŸ“Š å…±ç”Ÿæˆ {len(all_chunks)} ä¸ªè®­ç»ƒå—")

    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    import random
    random.shuffle(all_chunks)

    val_size = int(len(all_chunks) * VAL_SPLIT)
    train_chunks = all_chunks[val_size:]
    val_chunks = all_chunks[:val_size]

    print(f"  è®­ç»ƒé›†: {len(train_chunks)} æ¡")
    print(f"  éªŒè¯é›†: {len(val_chunks)} æ¡")

    # ä¿å­˜æ•°æ®
    train_file = train_dir / "train.jsonl"
    val_file = val_dir / "val.jsonl"

    train_dir.mkdir(parents=True, exist_ok=True)
    val_dir.mkdir(parents=True, exist_ok=True)

    print(f"\nğŸ’¾ ä¿å­˜æ•°æ®...")

    with open(train_file, 'w', encoding='utf-8') as f:
        for chunk in train_chunks:
            f.write(json.dumps(chunk, ensure_ascii=False) + '\n')

    with open(val_file, 'w', encoding='utf-8') as f:
        for chunk in val_chunks:
            f.write(json.dumps(chunk, ensure_ascii=False) + '\n')

    print(f"âœ“ è®­ç»ƒé›†: {train_file}")
    print(f"âœ“ éªŒè¯é›†: {val_file}")

    # æ˜¾ç¤ºæ•°æ®ç»Ÿè®¡
    print(f"\nğŸ“ˆ æ•°æ®ç»Ÿè®¡:")
    total_chars = sum(len(c['text']) for c in all_chunks)
    avg_chars = total_chars // len(all_chunks) if all_chunks else 0
    print(f"  æ€»å­—ç¬¦æ•°: {total_chars:,}")
    print(f"  å¹³å‡å—å¤§å°: {avg_chars:,} å­—ç¬¦")

if __name__ == "__main__":
    main()
PYTHON_EOF

python3 /tmp/prepare_data_general.py

# 5. æ›´æ–°é…ç½®
print_step "5/5" "ç”Ÿæˆé…ç½®ä¿¡æ¯..."

echo -e "\n${GREEN}========================================${NC}"
echo -e "${GREEN}  æ•°æ®å‡†å¤‡å®Œæˆï¼${NC}"
echo -e "${GREEN}========================================${NC}"
echo -e "${CYAN}æ•°æ®ä½ç½®:${NC}"
echo -e "  åŸå§‹æ•°æ®: $RAW_DIR"
echo -e "  è®­ç»ƒé›†:   $TRAIN_DIR/train.jsonl"
echo -e "  éªŒè¯é›†:   $VAL_DIR/val.jsonl"
echo -e "\n${YELLOW}å¼€å§‹è®­ç»ƒ:${NC}"
echo -e "  python start.py train \\"
echo -e "    --train-data $TRAIN_DIR/train.jsonl \\"
echo -e "    --val-data $VAL_DIR/val.jsonl \\"
echo -e "    --output-dir ./checkpoints/${OUTPUT_NAME}_model"
