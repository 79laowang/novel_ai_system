"""
Gradio WebUI - ä¸­æ–‡å°è¯´å†™ä½œAIç³»ç»Ÿäº¤äº’ç•Œé¢
æ”¯æŒå®æ—¶ç”Ÿæˆã€è®°å¿†ç®¡ç†ã€å‚æ•°è°ƒæ•´
"""
import os
import sys
import asyncio
from pathlib import Path
from typing import List, Tuple, Optional, Dict, Any
import json

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

import gradio as gr
from rich.console import Console
from rich import print as rprint

# å¯¼å…¥æœ¬åœ°æ¨¡å—
from src.inference.backend_factory import create_generator, get_generator, get_generator_sync
from src.memory.memory_manager import NovelMemoryManager, get_memory_manager

console = Console()


class NovelWebUI:
    """å°è¯´å†™ä½œWebUI"""

    def __init__(self, config):
        self.config = config
        self.generator = None  # ç±»å‹æ ¹æ®åç«¯åŠ¨æ€ç¡®å®š
        self.memory_manager: Optional[NovelMemoryManager] = None
        self.current_session: List[Dict[str, str]] = []

    async def initialize(self, lora_path: Optional[str] = None):
        """åˆå§‹åŒ–ç»„ä»¶"""
        rprint("[bold cyan]æ­£åœ¨åˆå§‹åŒ–WebUIç»„ä»¶...[/bold cyan]")

        # æ˜¾ç¤ºæ¨ç†åç«¯
        backend = self.config.model.inference_backend
        rprint(f"[cyan]æ¨ç†åç«¯: {backend}[/cyan]")

        # åˆå§‹åŒ–ç”Ÿæˆå™¨ï¼ˆä½¿ç”¨å·¥å‚å‡½æ•°ï¼‰
        self.generator = create_generator(self.config)

        # æ ¹æ®ç”Ÿæˆå™¨ç±»å‹é€‰æ‹©åˆå§‹åŒ–æ–¹å¼
        import inspect
        init_method = self.generator.initialize
        if inspect.iscoroutinefunction(init_method):
            await self.generator.initialize(lora_path)
        else:
            self.generator.initialize(lora_path)

        # åˆå§‹åŒ–è®°å¿†ç®¡ç†å™¨
        self.memory_manager = NovelMemoryManager(self.config)
        self.memory_manager.initialize()

        rprint("[bold green]âœ“ WebUIç»„ä»¶åˆå§‹åŒ–å®Œæˆ[/bold green]")

    def _create_generate_tab(self) -> gr.Blocks:
        """åˆ›å»ºç”Ÿæˆæ ‡ç­¾é¡µ"""
        with gr.Column() as tab:
            gr.Markdown("## ğŸ“ å°è¯´åˆ›ä½œ")

            # è¾“å…¥åŒºåŸŸ
            with gr.Row():
                with gr.Column(scale=3):
                    user_input = gr.Textbox(
                        label="åˆ›ä½œè¦æ±‚",
                        placeholder="è¯·æè¿°ä½ æƒ³è¦çš„å°è¯´å†…å®¹...",
                        lines=3,
                    )
                with gr.Column(scale=1):
                    memory_toggle = gr.Checkbox(
                        label="ä½¿ç”¨è®°å¿†",
                        value=True,
                        info="ä½¿ç”¨ä¹‹å‰çš„ä¸Šä¸‹æ–‡",
                    )

            # ç”Ÿæˆå‚æ•°
            with gr.Accordion("ç”Ÿæˆå‚æ•°", open=False):
                with gr.Row():
                    max_tokens = gr.Slider(
                        minimum=256,
                        maximum=4096,
                        value=2048,
                        step=256,
                        label="æœ€å¤§ç”Ÿæˆé•¿åº¦",
                    )
                    temperature = gr.Slider(
                        minimum=0.1,
                        maximum=2.0,
                        value=0.8,
                        step=0.1,
                        label="æ¸©åº¦ (éšæœºæ€§)",
                    )
                    top_p = gr.Slider(
                        minimum=0.1,
                        maximum=1.0,
                        value=0.95,
                        step=0.05,
                        label="Top-P",
                    )
                    top_k = gr.Slider(
                        minimum=1,
                        maximum=100,
                        value=50,
                        step=5,
                        label="Top-K",
                    )

            # ç”ŸæˆæŒ‰é’®
            with gr.Row():
                generate_btn = gr.Button("ğŸš€ ç”Ÿæˆ", variant="primary", size="lg")
                clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©º", variant="secondary")

            # è¾“å‡ºåŒºåŸŸ
            output = gr.Textbox(
                label="ç”Ÿæˆå†…å®¹",
                lines=15,
            )

            # ç”Ÿæˆå†å²
            history = gr.State([])

            # äº‹ä»¶ç»‘å®š - ä½¿ç”¨asyncå¤„ç†
            async def generate_novel_handler(input_text, toggle, max_tok, temp, top_p, top_k, hist):
                """å¼‚æ­¥ç”Ÿæˆå°è¯´ï¼ˆGradioåŸç”Ÿæ”¯æŒasyncï¼‰"""
                from datetime import datetime
                import traceback

                log_file = '/home/kewang/work/novel_ai_system/logs/debug.log'
                with open(log_file, 'a') as f:
                    f.write(f"\n[{datetime.now()}] [ç”Ÿæˆè¯·æ±‚] è¾“å…¥: {input_text[:100]}...\n")
                    f.write(f"[å‚æ•°] max_tokens={max_tok}, temp={temp}, top_p={top_p}, top_k={top_k}\n")

                try:
                    # ç›´æ¥è°ƒç”¨asyncæ–¹æ³•
                    result = await self.generator.generate_novel(
                        user_input=input_text,
                        max_tokens=max_tok,
                        temperature=temp,
                        top_p=top_p,
                        top_k=top_k,
                    )

                    with open(log_file, 'a') as f:
                        f.write(f"[ç”Ÿæˆå®Œæˆ] ç»“æœé•¿åº¦: {len(result)}\n")

                    return result, hist
                except Exception as e:
                    error_msg = f"ç”Ÿæˆå‡ºé”™: {str(e)}\n{traceback.format_exc()}"
                    with open(log_file, 'a') as f:
                        f.write(f"[é”™è¯¯] {error_msg}\n")
                    return error_msg, hist

            generate_btn.click(
                fn=generate_novel_handler,
                inputs=[user_input, memory_toggle, max_tokens, temperature, top_p, top_k, history],
                outputs=[output, history],
            )
            clear_btn.click(
                fn=lambda: ("", []),
                outputs=[output, history],
            )

        return tab

    def _create_memory_tab(self) -> gr.Blocks:
        """åˆ›å»ºè®°å¿†ç®¡ç†æ ‡ç­¾é¡µ"""
        with gr.Column() as tab:
            gr.Markdown("## ğŸ§  è®°å¿†ç®¡ç†")

            # è®°å¿†ç±»å‹é€‰æ‹©
            memory_type = gr.Radio(
                choices=[
                    ("ğŸ‘¤ äººç‰©è®°å¿†", "character"),
                    ("ğŸ“– æƒ…èŠ‚è®°å¿†", "plot"),
                    ("ğŸï¸ ç¯å¢ƒè®¾å®š", "setting"),
                    ("ğŸ’¬ é‡è¦å¯¹è¯", "dialogue"),
                    ("ğŸ“ æ•…äº‹ä¸Šä¸‹æ–‡", "context"),
                ],
                value="character",
                label="è®°å¿†ç±»å‹",
            )

            # æ·»åŠ è®°å¿†è¡¨å•
            with gr.Row():
                with gr.Column():
                    memory_name = gr.Textbox(label="åç§° (ä»…äººç‰©)", visible=True)
                    memory_content = gr.Textbox(
                        label="è®°å¿†å†…å®¹",
                        lines=3,
                        placeholder="è¾“å…¥è®°å¿†å†…å®¹...",
                    )
                    add_memory_btn = gr.Button("â• æ·»åŠ è®°å¿†", variant="primary")

                with gr.Column():
                    memory_list = gr.Textbox(
                        label="ç°æœ‰è®°å¿†",
                        lines=10,
                        interactive=False,
                    )
                    refresh_memory_btn = gr.Button("ğŸ”„ åˆ·æ–°")

            # æ£€ç´¢ç›¸å…³è®°å¿†
            with gr.Row():
                search_query = gr.Textbox(label="æœç´¢è®°å¿†", placeholder="è¾“å…¥å…³é”®è¯...")
                search_btn = gr.Button("ğŸ” æœç´¢")
            search_results = gr.Textbox(label="æœç´¢ç»“æœ", lines=5, interactive=False)

            # è®°å¿†æ“ä½œ
            with gr.Row():
                clear_memory_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºå½“å‰ç±»å‹è®°å¿†", variant="stop")
                export_memory_btn = gr.Button("ğŸ“¤ å¯¼å‡ºè®°å¿†", variant="secondary")
            export_output = gr.Textbox(label="å¯¼å‡ºç»“æœ", visible=False)

            # äº‹ä»¶ç»‘å®š
            add_memory_btn.click(
                fn=self.add_memory_wrapper,
                inputs=[memory_type, memory_name, memory_content],
                outputs=[memory_list],
            )
            refresh_memory_btn.click(
                fn=lambda mem_type: self.get_memory_list(mem_type),
                inputs=[memory_type],
                outputs=[memory_list],
            )
            search_btn.click(
                fn=self.search_memory_wrapper,
                inputs=[search_query],
                outputs=[search_results],
            )
            clear_memory_btn.click(
                fn=self.clear_memory_wrapper,
                inputs=[memory_type],
                outputs=[memory_list],
            )
            export_memory_btn.click(
                fn=self.export_memory_wrapper,
                outputs=[export_output],
            )

        return tab

    def _create_train_tab(self) -> gr.Blocks:
        """åˆ›å»ºè®­ç»ƒæ ‡ç­¾é¡µ"""
        with gr.Column() as tab:
            gr.Markdown("## ğŸ¯ æ¨¡å‹å¾®è°ƒ")

            # æ•°æ®é…ç½®
            with gr.Row():
                train_data_path = gr.Textbox(
                    label="è®­ç»ƒæ•°æ®è·¯å¾„",
                    placeholder="./data/train",
                    value="./data/train",
                )
                val_data_path = gr.Textbox(
                    label="éªŒè¯æ•°æ®è·¯å¾„ (å¯é€‰)",
                    placeholder="./data/val",
                )

            # è®­ç»ƒå‚æ•°
            with gr.Accordion("è®­ç»ƒå‚æ•°", open=False):
                with gr.Row():
                    num_epochs = gr.Slider(
                        minimum=1,
                        maximum=10,
                        value=3,
                        step=1,
                        label="è®­ç»ƒè½®æ•°",
                    )
                    batch_size = gr.Slider(
                        minimum=1,
                        maximum=8,
                        value=2,
                        step=1,
                        label="æ‰¹æ¬¡å¤§å°",
                    )
                    learning_rate = gr.Slider(
                        minimum=1e-5,
                        maximum=1e-3,
                        value=2e-4,
                        label="å­¦ä¹ ç‡",
                    )
                    lora_r = gr.Slider(
                        minimum=8,
                        maximum=128,
                        value=64,
                        step=8,
                        label="LoRA Rank",
                    )

            # è®­ç»ƒæ§åˆ¶
            with gr.Row():
                start_train_btn = gr.Button("ğŸš€ å¼€å§‹è®­ç»ƒ", variant="primary")
                stop_train_btn = gr.Button("â¹ï¸ åœæ­¢è®­ç»ƒ", variant="stop")

            # è®­ç»ƒçŠ¶æ€
            train_status = gr.Textbox(
                label="è®­ç»ƒçŠ¶æ€",
                lines=5,
                interactive=False,
                value="ç­‰å¾…å¼€å§‹è®­ç»ƒ...",
            )
            train_progress = gr.Progress()

            # äº‹ä»¶ç»‘å®š
            start_train_btn.click(
                fn=self.start_training_wrapper,
                inputs=[train_data_path, val_data_path, num_epochs, batch_size, learning_rate, lora_r],
                outputs=[train_status],
            )

        return tab

    def _create_settings_tab(self) -> gr.Blocks:
        """åˆ›å»ºè®¾ç½®æ ‡ç­¾é¡µ"""
        with gr.Column() as tab:
            gr.Markdown("## âš™ï¸ ç³»ç»Ÿè®¾ç½®")

            # æ¨¡å‹è®¾ç½®
            with gr.Group():
                gr.Markdown("### ğŸ¤– æ¨¡å‹é…ç½®")
                base_model = gr.Textbox(
                    label="åŸºç¡€æ¨¡å‹",
                    value=self.config.model.base_model,
                )
                lora_path = gr.Textbox(
                    label="LoRA æƒé‡è·¯å¾„ (å¯é€‰)",
                    placeholder="./checkpoints/final_model",
                )
                reload_model_btn = gr.Button("ğŸ”„ é‡æ–°åŠ è½½æ¨¡å‹", variant="primary")

            # ç³»ç»Ÿä¿¡æ¯
            with gr.Group():
                gr.Markdown("### ğŸ“Š ç³»ç»Ÿä¿¡æ¯")
                system_info = gr.Textbox(
                    label="ç³»ç»ŸçŠ¶æ€",
                    lines=8,
                    interactive=False,
                    value=self.get_system_info(),
                )
                refresh_info_btn = gr.Button("ğŸ”„ åˆ·æ–°ä¿¡æ¯")

            # äº‹ä»¶ç»‘å®š
            refresh_info_btn.click(
                fn=lambda: self.get_system_info(),
                outputs=[system_info],
            )

        return tab

    def build_ui(self) -> gr.Blocks:
        """æ„å»ºå®Œæ•´UI"""
        with gr.Blocks(
            title=self.config.webui.title,
        ) as app:
            # æ ‡é¢˜å’Œæè¿°
            gr.Markdown(
                f"""
                # {self.config.webui.title}

                {self.config.webui.description}

                ---
                """
            )

            # æ ‡ç­¾é¡µ
            with gr.Tabs():
                with gr.Tab("ğŸ“ åˆ›ä½œ"):
                    self._create_generate_tab()

                with gr.Tab("ğŸ§  è®°å¿†"):
                    self._create_memory_tab()

                with gr.Tab("ğŸ¯ è®­ç»ƒ"):
                    self._create_train_tab()

                with gr.Tab("âš™ï¸ è®¾ç½®"):
                    self._create_settings_tab()

            # é¡µè„š
            gr.Markdown(
                """
                ---

                ğŸ’¡ **æç¤º**: ä½¿ç”¨è®°å¿†åŠŸèƒ½å¯ä»¥è®©AIè®°ä½ä¹‹å‰çš„åˆ›ä½œå†…å®¹ï¼Œç”Ÿæˆæ›´è¿è´¯çš„æ•…äº‹ã€‚

                ğŸš€ **Powered by**: vLLM + LoRA + ChromaDB
                """
            )

        return app

    def _get_custom_css(self) -> str:
        """è·å–è‡ªå®šä¹‰CSS"""
        return """
        .gradio-container {
            max-width: 1400px !important;
        }
        .generate-btn {
            background: linear-gradient(45deg, #667eea 0%, #764ba2 100%) !important;
        }
        """

    # Wrapperå‡½æ•° (ç”¨äºå¼‚æ­¥è°ƒç”¨)
    async def generate_wrapper(
        self,
        user_input: str,
        use_memory: bool,
        max_tokens: int,
        temperature: float,
        top_p: float,
        top_k: int,
        history: List[Dict[str, str]],
    ) -> Tuple[str, List[Dict[str, str]]]:
        """ç”ŸæˆåŒ…è£…å™¨"""
        import logging
        logger = logging.getLogger(__name__)

        logger.info("[å¼‚æ­¥ç”Ÿæˆ] å‡½æ•°å¼€å§‹æ‰§è¡Œ")
        if not user_input.strip():
            logger.warning("[å¼‚æ­¥ç”Ÿæˆ] è¾“å…¥ä¸ºç©º")
            return "è¯·è¾“å…¥åˆ›ä½œè¦æ±‚", history

        # è·å–è®°å¿†ä¸Šä¸‹æ–‡
        memory_context = ""
        if use_memory and self.memory_manager:
            logger.info("[è®°å¿†ç³»ç»Ÿ] è·å–è®°å¿†ä¸Šä¸‹æ–‡...")
            memory_context = self.memory_manager.get_formatted_context(user_input)
            logger.info(f"[è®°å¿†ç³»ç»Ÿ] ä¸Šä¸‹æ–‡é•¿åº¦: {len(memory_context)} å­—ç¬¦")

        # ç”Ÿæˆå†…å®¹ï¼ˆæ”¯æŒå¼‚æ­¥å’ŒåŒæ­¥ç”Ÿæˆå™¨ï¼‰
        logger.info(f"[{self.config.model.inference_backend}] å¼€å§‹ç”Ÿæˆ...")

        # æ£€æŸ¥ç”Ÿæˆå™¨æ˜¯å¦æœ‰å¼‚æ­¥æ–¹æ³•
        if hasattr(self.generator, 'generate_novel_async'):
            result = await self.generator.generate_novel_async(
                user_input=user_input,
                context=memory_context,
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p,
                top_k=top_k,
            )
        else:
            # åŒæ­¥æ–¹æ³•ï¼Œç›´æ¥è°ƒç”¨
            result = self.generator.generate_novel(
                user_input=user_input,
                context=memory_context,
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p,
                top_k=top_k,
            )

        logger.info(f"[{self.config.model.inference_backend}] ç”Ÿæˆå®Œæˆï¼Œç»“æœé•¿åº¦: {len(result)} å­—ç¬¦")

        # ä¿å­˜åˆ°å†å²
        history.append({
            "user": user_input,
            "assistant": result,
            "timestamp": str(asyncio.get_event_loop().time()),
        })

        # ä¿å­˜åˆ°è®°å¿†
        if self.memory_manager:
            logger.info("[è®°å¿†ç³»ç»Ÿ] ä¿å­˜ç”Ÿæˆç»“æœåˆ°è®°å¿†...")
            self.memory_manager.summarize_session(result)

        logger.info("[å¼‚æ­¥ç”Ÿæˆ] å‡½æ•°æ‰§è¡Œå®Œæˆ")
        return result, history

    def generate_wrapper_sync(
        self,
        user_input: str,
        use_memory: bool,
        max_tokens: int,
        temperature: float,
        top_p: float,
        top_k: int,
        history: List[Dict[str, str]],
    ) -> Tuple[str, List[Dict[str, str]]]:
        """åŒæ­¥åŒ…è£…å™¨ï¼Œç”¨äºGradio"""
        import asyncio
        import logging
        from datetime import datetime

        # é…ç½®è°ƒè¯•æ—¥å¿—
        logging.basicConfig(
            level=logging.DEBUG,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('/home/kewang/work/novel_ai_system/logs/debug.log'),
                logging.StreamHandler()
            ]
        )
        logger = logging.getLogger(__name__)

        logger.info(f"[ç”Ÿæˆè¯·æ±‚] è¾“å…¥: {user_input[:100]}...")
        logger.info(f"[ç”Ÿæˆå‚æ•°] max_tokens={max_tokens}, temp={temperature}, top_p={top_p}, top_k={top_k}")

        # è·å–æˆ–åˆ›å»ºäº‹ä»¶å¾ªç¯
        try:
            loop = asyncio.get_event_loop()
            if loop.is_closed():
                raise RuntimeError("Loop is closed")
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)

        try:
            logger.info("[äº‹ä»¶å¾ªç¯] å¼€å§‹è¿è¡Œå¼‚æ­¥ç”Ÿæˆ...")
            start_time = datetime.now()

            # è¿è¡Œå¼‚æ­¥å‡½æ•°
            result = loop.run_until_complete(
                self.generate_wrapper(
                    user_input, use_memory, max_tokens,
                    temperature, top_p, top_k, history
                )
            )

            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            logger.info(f"[ç”Ÿæˆå®Œæˆ] è€—æ—¶: {duration:.2f}ç§’")
            logger.info(f"[ç”Ÿæˆç»“æœ] é•¿åº¦: {len(result[0])} å­—ç¬¦")

            return result
        except Exception as e:
            logger.error(f"[ç”Ÿæˆé”™è¯¯] {type(e).__name__}: {e}", exc_info=True)
            return f"ç”Ÿæˆå‡ºé”™: {str(e)}\n\nè¯·æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶: /home/kewang/work/novel_ai_system/logs/debug.log", history

    def add_memory_wrapper(
        self,
        memory_type: str,
        memory_name: str,
        memory_content: str,
    ) -> str:
        """æ·»åŠ è®°å¿†åŒ…è£…å™¨"""
        if not memory_content.strip():
            return "è¯·è¾“å…¥è®°å¿†å†…å®¹"

        if memory_type == "character":
            self.memory_manager.add_character(
                name=memory_name or "æœªå‘½å",
                description=memory_content,
            )
        elif memory_type == "plot":
            self.memory_manager.add_plot(memory_content)
        elif memory_type == "setting":
            self.memory_manager.add_setting(memory_content)
        elif memory_type == "dialogue":
            self.memory_manager.add_dialogue(memory_content, speaker=memory_name)
        else:
            self.memory_manager.add_memory(memory_content, memory_type="context")

        return self.get_memory_list(memory_type)

    def get_memory_list(self, memory_type: str) -> str:
        """è·å–è®°å¿†åˆ—è¡¨"""
        memories = self.memory_manager.get_all_memories(memory_type)
        if not memories:
            return "æš‚æ— è®°å¿†"

        result = []
        for i, mem in enumerate(memories, 1):
            result.append(f"{i}. {mem.get('content', mem.get('description', ''))[:100]}")

        return "\n\n".join(result)

    def search_memory_wrapper(self, query: str) -> str:
        """æœç´¢è®°å¿†åŒ…è£…å™¨"""
        if not query.strip():
            return "è¯·è¾“å…¥æœç´¢å…³é”®è¯"

        results = self.memory_manager.retrieve_memory(query, top_k=5)
        if not results:
            return "æœªæ‰¾åˆ°ç›¸å…³è®°å¿†"

        output = []
        for i, res in enumerate(results, 1):
            output.append(
                f"{i}. [{res['type']}] {res['content'][:100]}\n"
                f"   ç›¸å…³åº¦: {1-res['distance']:.2f}"
            )

        return "\n\n".join(output)

    def clear_memory_wrapper(self, memory_type: str) -> str:
        """æ¸…é™¤è®°å¿†åŒ…è£…å™¨"""
        self.memory_manager.clear_memories(memory_type)
        return "å·²æ¸…ç©ºè®°å¿†"

    def export_memory_wrapper(self):
        """å¯¼å‡ºè®°å¿†åŒ…è£…å™¨"""
        import json
        memories = self.memory_manager.get_all_memories()
        return json.dumps(memories, ensure_ascii=False, indent=2)

    def start_training_wrapper(
        self,
        train_data_path: str,
        val_data_path: str,
        num_epochs: int,
        batch_size: int,
        learning_rate: float,
        lora_r: int,
    ) -> str:
        """å¼€å§‹è®­ç»ƒåŒ…è£…å™¨"""
        return "è®­ç»ƒåŠŸèƒ½éœ€è¦å•ç‹¬è¿è¡Œ train_lora.py\n\n" \
               f"é…ç½®:\n" \
               f"- æ•°æ®è·¯å¾„: {train_data_path}\n" \
               f"- è®­ç»ƒè½®æ•°: {num_epochs}\n" \
               f"- æ‰¹æ¬¡å¤§å°: {batch_size}\n" \
               f"- å­¦ä¹ ç‡: {learning_rate}\n" \
               f"- LoRA Rank: {lora_r}"

    def get_system_info(self) -> str:
        """è·å–ç³»ç»Ÿä¿¡æ¯"""
        import torch
        import subprocess

        info = []

        # GPUä¿¡æ¯
        if torch.cuda.is_available():
            gpu_count = torch.cuda.device_count()
            info.append(f"GPU: {gpu_count} x {torch.cuda.get_device_name(0)}")
            info.append(f"æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        else:
            info.append("GPU: ä¸å¯ç”¨")

        # å†…å­˜ä¿¡æ¯
        result = subprocess.run(['free', '-h'], capture_output=True, text=True)
        info.append(f"å†…å­˜: {result.stdout.split()[7]}")

        # æ¨¡å‹ä¿¡æ¯
        info.append(f"åŸºç¡€æ¨¡å‹: {self.config.model.base_model}")

        # è®°å¿†ç»Ÿè®¡
        if self.memory_manager:
            total_memories = sum(len(v) for v in self.memory_manager.long_term_memory.values())
            info.append(f"è®°å¿†æ•°é‡: {total_memories} æ¡")

        return "\n".join(info)


# å…¨å±€å®ä¾‹
_webui: Optional[NovelWebUI] = None
_engine_event_loop = None  # ä¿å­˜å¼•æ“çš„äº‹ä»¶å¾ªç¯


async def launch_webui(lora_path: Optional[str] = None):
    """å¯åŠ¨WebUI - ä¿æŒäº‹ä»¶å¾ªç¯è¿è¡Œ"""
    from config import config
    global _engine_event_loop

    # è·å–å½“å‰äº‹ä»¶å¾ªç¯ï¼ˆå°†ç”¨äºå¼•æ“ï¼‰
    _engine_event_loop = asyncio.get_running_loop()

    global _webui
    _webui = NovelWebUI(config)
    await _webui.initialize(lora_path)

    # æ„å»ºUI
    app = _webui.build_ui()

    # å¯åŠ¨ - ä¸é˜»å¡çº¿ç¨‹ï¼Œè®©äº‹ä»¶å¾ªç¯ç»§ç»­è¿è¡Œ
    app.launch(
        server_name=config.webui.host,
        server_port=config.webui.port,
        share=config.webui.share,
        show_error=True,
        prevent_thread_lock=True,  # å…³é”®ï¼šä¸é˜»å¡äº‹ä»¶å¾ªç¯
        theme=gr.themes.Soft(),
        css=_webui._get_custom_css(),
    )

    # ä¿æŒäº‹ä»¶å¾ªç¯è¿è¡Œ
    try:
        # æ— é™ç­‰å¾…ï¼Œä¿æŒäº‹ä»¶å¾ªç¯æ´»è·ƒ
        await asyncio.Future()
    except KeyboardInterrupt:
        print("\næ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨å…³é—­...")


def main():
    """ä¸»å…¥å£"""
    import argparse

    parser = argparse.ArgumentParser(description="ä¸­æ–‡å°è¯´å†™ä½œAIç³»ç»Ÿ - WebUI")
    parser.add_argument("--lora", type=str, default=None, help="LoRAæƒé‡è·¯å¾„")
    parser.add_argument("--host", type=str, default=None, help="æœåŠ¡å™¨åœ°å€")
    parser.add_argument("--port", type=int, default=None, help="æœåŠ¡å™¨ç«¯å£")
    parser.add_argument("--share", action="store_true", help="åˆ›å»ºå…¬å…±é“¾æ¥")

    args = parser.parse_args()

    # æ›´æ–°é…ç½®
    from config import config
    if args.host:
        config.webui.host = args.host
    if args.port:
        config.webui.port = args.port
    if args.share:
        config.webui.share = True

    # å¯åŠ¨
    asyncio.run(launch_webui(lora_path=args.lora))


if __name__ == "__main__":
    main()
